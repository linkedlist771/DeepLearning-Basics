{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 导入需要的库"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functions as f\n",
    "from gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "from layers import *\n",
    "import pickle\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义读取数据集的函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "\n",
    "def load_mnist_data(kind):\n",
    "    '''\n",
    "    加载数据集\n",
    "    :param kind: 加载训练数据还是测试数据\n",
    "    :return: 打平之后的数据和one hot编码的标签\n",
    "    '''\n",
    "    labels_path = '../data/%s-labels-idx1-ubyte' % kind\n",
    "    images_path = '../data/%s-images-idx3-ubyte' % kind\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images / 255., np.eye(10)[labels]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义神经网络架构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "\n",
    "class NLayerMLP:\n",
    "\n",
    "    def __init__(self, sizes, weight_init_std = 0):\n",
    "        # 存储权重值和偏置值\n",
    "        self.sizes = sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for input_size, output_size in zip(sizes[0:-1], sizes[1:]):\n",
    "            # 初始化权重\n",
    "            weight = weight_init_std * np.random.randn(input_size, output_size)\n",
    "            bias = np.zeros(output_size)\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "        # 这个神经网络的层数为n-1\n",
    "        size = len(sizes)-1\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        for i in range(0, size):\n",
    "            if i != size-1:\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "                #self.layers['Relu1'] = Relu()\n",
    "                self.layers[f'Sigmoid1{i}'] = Sigmoid()\n",
    "            else:\n",
    "                # 这里没有激活函数\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        #self.lastLayer = IdentityWithLoss()\n",
    "        self.params = dict()\n",
    "        self.params['sizes'] = self.sizes\n",
    "        self.params['biases'] = self.biases\n",
    "        self.params['weights'] = self.weights\n",
    "        self.params['layers'] = self.layers\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'] = numerical_gradient(loss_W, self.weights[i])\n",
    "            grads[f'b{i}'] = numerical_gradient(loss_W, self.biases[i])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'], grads[f'b{i}'] = self.layers[f'Affine{i}'].dW, self.layers[f'Affine{i}'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def sgd(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None,\n",
    "            loss_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        for i in range(epochs):\n",
    "            for batch_index in range(0, train_size, batch_size):\n",
    "                lower_range = batch_index\n",
    "                upper_range = batch_index + batch_size\n",
    "                if upper_range > train_size:\n",
    "                    upper_range = train_size\n",
    "                x_batch = train_features[lower_range: upper_range, :]\n",
    "                y_batch = train_labels[lower_range: upper_range]\n",
    "                # 计算梯度\n",
    "                #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "                grad = self.gradient(x_batch, y_batch)\n",
    "                # 更新参数\n",
    "                for index in range(len(network.sizes)-1):\n",
    "                    self.weights[index] -= lr * grad[f\"W{index}\"]\n",
    "                    self.biases[index] -= lr * grad[f\"b{index}\"]\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "                if loss_path:\n",
    "                    with open(loss_path, \"a\") as f:\n",
    "                        if i == 0:\n",
    "                            f.write(\"epoch train_loss test_loss  accuracy\\n\")\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "\n",
    "\n",
    "    def adam(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None,\n",
    "             loss_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        for i in range(epochs):\n",
    "            for batch_index in range(0, train_size, batch_size):\n",
    "                lower_range = batch_index\n",
    "                upper_range = batch_index + batch_size\n",
    "                if upper_range > train_size:\n",
    "                    upper_range = train_size\n",
    "                x_batch = train_features[lower_range: upper_range, :]\n",
    "                y_batch = train_labels[lower_range: upper_range]\n",
    "                # 计算梯度\n",
    "                #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "                grad = self.gradient(x_batch, y_batch)\n",
    "                # 更新参数, 这里使用adam算法\n",
    "                rho1, rho2 = 0.9, 0.999\n",
    "                delta = 1e-8\n",
    "                for index in range(len(network.sizes)-1):\n",
    "                    weight = self.weights[index]\n",
    "                    weight_grad = grad[f\"W{index}\"]\n",
    "                    s = np.zeros_like(weight)\n",
    "                    r = np.zeros_like(weight)\n",
    "                    # update the first and the second moment\n",
    "                    s = rho1 * s + (1 - rho1) * weight_grad\n",
    "                    r = rho2 * r + (1 - rho2) * np.square(weight_grad)\n",
    "                    # get the partial first and second moment\n",
    "                    s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                    r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                    # use the partial first and second moment to modify the GD method\n",
    "                    self.weights[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "                    bias = self.biases[index]\n",
    "                    bias_grad = grad[f\"b{index}\"]\n",
    "                    s = np.zeros_like(bias)\n",
    "                    r = np.zeros_like(bias)\n",
    "                    # update the first and the second moment\n",
    "                    s = rho1 * s + (1 - rho1) * bias_grad\n",
    "                    r = rho2 * r + (1 - rho2) * np.square(bias_grad)\n",
    "                    # get the partial first and second moment\n",
    "                    s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                    r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                    # use the partial first and second moment to modify the GD method\n",
    "                    self.biases[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "                if loss_path:\n",
    "                    with open(loss_path, \"a\") as f:\n",
    "                        if i == 0:\n",
    "                            f.write(\"epoch train_loss test_loss  accuracy\\n\")\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "\n",
    "    # 保存权重文件\n",
    "    def save(self,path):\n",
    "        with open(path,'wb') as f :\n",
    "            self.params['sizes'] = self.sizes\n",
    "            self.params['biases'] = self.biases\n",
    "            self.params['weights'] = self.weights\n",
    "            self.params['layers'] = self.layers\n",
    "            pickle.dump(self.params, f)\n",
    "\n",
    "    # 导入权重文件\n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f :\n",
    "            self.params = pickle.load(f)\n",
    "            self.sizes = self.params['sizes']\n",
    "            self.biases = self.params['biases']\n",
    "            self.weights = self.params['weights']\n",
    "            self.layers = self.params['layers']\n",
    "\n",
    "\n",
    "        #print(weight)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 导入训练集和测试集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_images, train_labels = load_mnist_data(kind='train')\n",
    "train_images = np.reshape(train_images, newshape=(-1, 1, 784))\n",
    "test_images, test_labels = load_mnist_data('t10k')\n",
    "test_images = np.reshape(test_images, newshape=(-1, 1, 784))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "training_set_size = len(train_images)\n",
    "testing_set_size = len(test_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0:8.575984678652584e-13\n",
      "b0:1.3007690419456141e-12\n",
      "W1:7.20684311049466e-11\n",
      "b1:1.412010317697332e-10\n",
      "W2:1.1215424440975389e-08\n",
      "b2:2.1963221961225284e-08\n"
     ]
    }
   ],
   "source": [
    "## check gradient\n",
    "\n",
    "network = NLayerMLP(sizes=[784, 32, 2, 10])\n",
    "mini_batch_size = 100\n",
    "\n",
    "grad_numerical = network.numerical_gradient(train_images[:mini_batch_size], train_labels[:mini_batch_size])\n",
    "grad_backprop = network.gradient(train_images[:mini_batch_size], train_labels[:mini_batch_size])\n",
    "\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 95.84%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |0.21743682088681396 , 0.17378501691924672, 95.75%\n",
      "Weight has been saved !!\n",
      "epoch: 10, train loss, test loss , accuracy |0.2110085962507403 , 0.18207387568266728, 95.69%\n",
      "Weight has been saved !!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [45]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch: , train loss, test loss , accuracy |, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight has been saved !!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtest_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miter_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m            \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_save_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.adam\u001B[1;34m(self, training_data, epochs, batch_size, lr, test_data, iter_num, save_path, loss_path)\u001B[0m\n\u001B[0;32m    139\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m train_labels[lower_range: upper_range]\n\u001B[0;32m    140\u001B[0m \u001B[38;5;66;03m# 计算梯度\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;66;03m#grad = self.numerical_gradient(x_batch, y_batch)\u001B[39;00m\n\u001B[1;32m--> 142\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;66;03m# 更新参数, 这里使用adam算法\u001B[39;00m\n\u001B[0;32m    144\u001B[0m rho1, rho2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.9\u001B[39m, \u001B[38;5;241m0.999\u001B[39m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.gradient\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     73\u001B[0m layers\u001B[38;5;241m.\u001B[39mreverse()\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m layers:\n\u001B[1;32m---> 75\u001B[0m     dout \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;66;03m# 设定\u001B[39;00m\n\u001B[0;32m     78\u001B[0m grads \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32mE:\\桌面\\深度学习数学基础\\DeepLearning-Basics\\MNIST识别实现\\Mnist MLP\\layers.py:61\u001B[0m, in \u001B[0;36mAffine.backward\u001B[1;34m(self, dout)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackward\u001B[39m(\u001B[38;5;28mself\u001B[39m, dout):\n\u001B[1;32m---> 61\u001B[0m     dx \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdW \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx\u001B[38;5;241m.\u001B[39mT, dout)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdb \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(dout, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构\n",
    "network = NLayerMLP(sizes=[784, 64, 32, 10])\n",
    "weight_save_path  = \"weight_multi_layer_adam.pickle\"\n",
    "epoches = 100000  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "iter_num = 10\n",
    "load_weight = True\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.adam(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=10,\n",
    "            save_path=weight_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 验证权重和偏置都为0时"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 661, train loss, test loss , accuracy |1.6799289539170992 , 1.5306884115579271, 38.75%\n",
      "Weight has been saved !!\n",
      "epoch: 662, train loss, test loss , accuracy |1.6799115628929446 , 1.53066593287219, 38.75%\n",
      "Weight has been saved !!\n",
      "epoch: 663, train loss, test loss , accuracy |1.6798941480787288 , 1.530643524834076, 38.75%\n",
      "Weight has been saved !!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [52]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch: , train loss, test loss , accuracy |, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight has been saved !!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msgd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m             \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtest_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miter_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43miter_num\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m             \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_save_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msingle_layer_numpy_sgd.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.sgd\u001B[1;34m(self, training_data, epochs, batch_size, lr, test_data, iter_num, save_path, loss_path)\u001B[0m\n\u001B[0;32m     99\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m train_labels[lower_range: upper_range]\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# 计算梯度\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m#grad = self.numerical_gradient(x_batch, y_batch)\u001B[39;00m\n\u001B[1;32m--> 102\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# 更新参数\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(network\u001B[38;5;241m.\u001B[39msizes)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.gradient\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgradient\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# backward\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     dout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.loss\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloss\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[1;32m---> 42\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlastLayer\u001B[38;5;241m.\u001B[39mforward(y, t)\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.predict\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m---> 36\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mE:\\桌面\\深度学习数学基础\\DeepLearning-Basics\\MNIST识别实现\\Mnist MLP\\layers.py:56\u001B[0m, in \u001B[0;36mAffine.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     53\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m---> 56\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构\n",
    "network = NLayerMLP(sizes=[784, 128, 10])\n",
    "weight_save_path  = \"weight_64_hidden.pickle\"\n",
    "epoches = 10000  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-2\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "iter_num = 1\n",
    "load_weight = False\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.sgd(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "             lr=learning_rate, test_data=(test_images, test_labels), iter_num=iter_num,\n",
    "             save_path=weight_save_path, loss_path=\"single_layer_numpy_sgd.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 20.75%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |2.1283100007268407 , 2.076106760557496, 20.75%\n",
      "Weight has been saved !!\n",
      "epoch: 1, train loss, test loss , accuracy |2.1282113165164653 , 2.076112322932562, 20.75%\n",
      "Weight has been saved !!\n",
      "epoch: 2, train loss, test loss , accuracy |2.1281253676766254 , 2.0761171416420927, 20.74%\n",
      "Weight has been saved !!\n",
      "epoch: 3, train loss, test loss , accuracy |2.1280514256212837 , 2.0761211863424385, 20.74%\n",
      "Weight has been saved !!\n",
      "epoch: 4, train loss, test loss , accuracy |2.127988800356441 , 2.0761244284869242, 20.74%\n",
      "Weight has been saved !!\n",
      "epoch: 5, train loss, test loss , accuracy |2.1279368388675697 , 2.076126840473701, 20.73%\n",
      "Weight has been saved !!\n",
      "epoch: 6, train loss, test loss , accuracy |2.1278949235170113 , 2.07612839490973, 20.73%\n",
      "Weight has been saved !!\n",
      "epoch: 7, train loss, test loss , accuracy |2.1278624704614657 , 2.0761290640082795, 20.73%\n",
      "Weight has been saved !!\n",
      "epoch: 8, train loss, test loss , accuracy |2.127838928092426 , 2.0761288191388045, 20.73%\n",
      "Weight has been saved !!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [49]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch: , train loss, test loss , accuracy |, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight has been saved !!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msgd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtest_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miter_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43miter_num\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m            \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_save_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msingle_layer_numpy_adam4.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.sgd\u001B[1;34m(self, training_data, epochs, batch_size, lr, test_data, iter_num, save_path, loss_path)\u001B[0m\n\u001B[0;32m     99\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m train_labels[lower_range: upper_range]\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# 计算梯度\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m#grad = self.numerical_gradient(x_batch, y_batch)\u001B[39;00m\n\u001B[1;32m--> 102\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# 更新参数\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(network\u001B[38;5;241m.\u001B[39msizes)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.gradient\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgradient\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# backward\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     dout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.loss\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloss\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[1;32m---> 42\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlastLayer\u001B[38;5;241m.\u001B[39mforward(y, t)\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.predict\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m---> 36\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mE:\\桌面\\深度学习数学基础\\DeepLearning-Basics\\MNIST识别实现\\Mnist MLP\\layers.py:56\u001B[0m, in \u001B[0;36mAffine.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     53\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m---> 56\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构\n",
    "network = NLayerMLP(sizes=[784, 32,4, 10])\n",
    "weight_save_path  = \"weight_64_hidden.pickle\"\n",
    "epoches = 100000  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "iter_num = 1\n",
    "load_weight = True\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.sgd(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=iter_num,\n",
    "            save_path=weight_save_path, loss_path=\"single_layer_numpy_adam4.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae51ee3d492f24e83e77a52eb34bf16365894f8747390aa8e17995579dedf394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
