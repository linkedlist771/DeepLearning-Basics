{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 导入需要的库"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functions as f\n",
    "from gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "from layers import *\n",
    "import pickle\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义读取数据集的函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "def load_mnist_data(kind):\n",
    "    '''\n",
    "    加载数据集\n",
    "    :param kind: 加载训练数据还是测试数据\n",
    "    :return: 打平之后的数据和one hot编码的标签\n",
    "    '''\n",
    "    labels_path = '../data/%s-labels-idx1-ubyte' % kind\n",
    "    images_path = '../data/%s-images-idx3-ubyte' % kind\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images / 255., np.eye(10)[labels]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 定义神经网络架构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "\n",
    "class NLayerMLP:\n",
    "\n",
    "    def __init__(self, sizes, weight_init_std = 0.01):\n",
    "        # 存储权重值和偏置值\n",
    "        self.sizes = sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for input_size, output_size in zip(sizes[0:-1], sizes[1:]):\n",
    "            # 初始化权重\n",
    "            weight = weight_init_std * np.random.randn(input_size, output_size)\n",
    "            bias = np.zeros(output_size)\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "        # 这个神经网络的层数为n-1\n",
    "        size = len(sizes)-1\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        for i in range(0, size):\n",
    "            if i != size-1:\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "                #self.layers['Relu1'] = Relu()\n",
    "                self.layers[f'Sigmoid1{i}'] = Sigmoid()\n",
    "            else:\n",
    "                # 这里没有激活函数\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        #self.lastLayer = IdentityWithLoss()\n",
    "        self.params = dict()\n",
    "        self.params['sizes'] = self.sizes\n",
    "        self.params['biases'] = self.biases\n",
    "        self.params['weights'] = self.weights\n",
    "        self.params['layers'] = self.layers\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'] = numerical_gradient(loss_W, self.weights[i])\n",
    "            grads[f'b{i}'] = numerical_gradient(loss_W, self.biases[i])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'], grads[f'b{i}'] = self.layers[f'Affine{i}'].dW, self.layers[f'Affine{i}'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def sgd(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        for i in range(epochs):\n",
    "            batch_mask = np.random.choice(train_size, batch_size)\n",
    "            x_batch = train_features[batch_mask]\n",
    "            y_batch = train_labels[batch_mask]\n",
    "            # 计算梯度\n",
    "            #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "            grad = self.gradient(x_batch, y_batch)\n",
    "            # 更新参数\n",
    "            for index in range(len(network.sizes)-1):\n",
    "                self.weights[index] -= lr * grad[f\"W{index}\"]\n",
    "                self.biases[index] -= lr * grad[f\"b{index}\"]\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "\n",
    "    def adam(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        for i in range(epochs):\n",
    "            batch_mask = np.random.choice(train_size, batch_size)\n",
    "            x_batch = train_features[batch_mask]\n",
    "            y_batch = train_labels[batch_mask]\n",
    "            # 计算梯度\n",
    "            #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "            grad = self.gradient(x_batch, y_batch)\n",
    "            # 更新参数, 这里使用adam算法\n",
    "            rho1, rho2 = 0.9, 0.999\n",
    "            delta = 1e-8\n",
    "            for index in range(len(network.sizes)-1):\n",
    "                weight = self.weights[index]\n",
    "                weight_grad = grad[f\"W{index}\"]\n",
    "                s = np.zeros_like(weight)\n",
    "                r = np.zeros_like(weight)\n",
    "                # update the first and the second moment\n",
    "                s = rho1 * s + (1 - rho1) * weight_grad\n",
    "                r = rho2 * r + (1 - rho2) * np.square(weight_grad)\n",
    "                # get the partial first and second moment\n",
    "                s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                # use the partial first and second moment to modify the GD method\n",
    "                self.weights[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "                bias = self.biases[index]\n",
    "                bias_grad = grad[f\"b{index}\"]\n",
    "                s = np.zeros_like(bias)\n",
    "                r = np.zeros_like(bias)\n",
    "                # update the first and the second moment\n",
    "                s = rho1 * s + (1 - rho1) * bias_grad\n",
    "                r = rho2 * r + (1 - rho2) * np.square(bias_grad)\n",
    "                # get the partial first and second moment\n",
    "                s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                # use the partial first and second moment to modify the GD method\n",
    "                self.biases[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "\n",
    "    # 保存权重文件\n",
    "    def save(self,path):\n",
    "        with open(path,'wb') as f :\n",
    "            self.params['sizes'] = self.sizes\n",
    "            self.params['biases'] = self.biases\n",
    "            self.params['weights'] = self.weights\n",
    "            self.params['layers'] = self.layers\n",
    "            pickle.dump(self.params, f)\n",
    "\n",
    "    # 导入权重文件\n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f :\n",
    "            self.params = pickle.load(f)\n",
    "            self.sizes = self.params['sizes']\n",
    "            self.biases = self.params['biases']\n",
    "            self.weights = self.params['weights']\n",
    "            self.layers = self.params['layers']\n",
    "\n",
    "\n",
    "        #print(weight)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 导入训练集和测试集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train_images, train_labels = load_mnist_data(kind='train')\n",
    "train_images = np.reshape(train_images, newshape=(-1, 1, 784))\n",
    "test_images, test_labels = load_mnist_data('t10k')\n",
    "test_images = np.reshape(test_images, newshape=(-1, 1, 784))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "training_set_size = len(train_images)\n",
    "testing_set_size = len(test_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0:8.575984678652584e-13\n",
      "b0:1.3007690419456141e-12\n",
      "W1:7.20684311049466e-11\n",
      "b1:1.412010317697332e-10\n",
      "W2:1.1215424440975389e-08\n",
      "b2:2.1963221961225284e-08\n"
     ]
    }
   ],
   "source": [
    "## check gradient\n",
    "\n",
    "network = NLayerMLP(sizes=[784, 32, 2, 10])\n",
    "mini_batch_size = 100\n",
    "\n",
    "grad_numerical = network.numerical_gradient(train_images[:mini_batch_size], train_labels[:mini_batch_size])\n",
    "grad_backprop = network.gradient(train_images[:mini_batch_size], train_labels[:mini_batch_size])\n",
    "\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 670, train loss, test loss , accuracy |0.23748128061198914 , 0.2915267311076135, 91.94%\n",
      "Weight has been saved !!\n",
      "epoch: 680, train loss, test loss , accuracy |0.15569014606630033 , 0.2886425815330819, 91.93%\n",
      "Weight has been saved !!\n",
      "epoch: 690, train loss, test loss , accuracy |0.2687326329898384 , 0.2861199217265199, 92.09%\n",
      "Weight has been saved !!\n",
      "epoch: 700, train loss, test loss , accuracy |0.2813681721509769 , 0.2856840986093324, 91.99000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 710, train loss, test loss , accuracy |0.2442431996017581 , 0.2803295818484198, 92.34%\n",
      "Weight has been saved !!\n",
      "epoch: 720, train loss, test loss , accuracy |0.29824549477828516 , 0.2797040175708415, 92.47%\n",
      "Weight has been saved !!\n",
      "epoch: 730, train loss, test loss , accuracy |0.1478036893304196 , 0.2804218663610481, 92.39%\n",
      "Weight has been saved !!\n",
      "epoch: 740, train loss, test loss , accuracy |0.20156303257227676 , 0.2867395138345321, 92.02%\n",
      "Weight has been saved !!\n",
      "epoch: 750, train loss, test loss , accuracy |0.38572688287614826 , 0.27795347592679726, 92.23%\n",
      "Weight has been saved !!\n",
      "epoch: 760, train loss, test loss , accuracy |0.2765930678720395 , 0.2752721194552062, 92.33%\n",
      "Weight has been saved !!\n",
      "epoch: 770, train loss, test loss , accuracy |0.1278246582016384 , 0.2706137039660743, 92.47%\n",
      "Weight has been saved !!\n",
      "epoch: 780, train loss, test loss , accuracy |0.2473384895114076 , 0.28733107278687664, 91.78%\n",
      "Weight has been saved !!\n",
      "epoch: 790, train loss, test loss , accuracy |0.21711715257183173 , 0.26687569282114476, 92.62%\n",
      "Weight has been saved !!\n",
      "epoch: 800, train loss, test loss , accuracy |0.20724113537120925 , 0.2659699097795821, 92.78%\n",
      "Weight has been saved !!\n",
      "epoch: 810, train loss, test loss , accuracy |0.2071934282233565 , 0.26368262336584497, 92.72%\n",
      "Weight has been saved !!\n",
      "epoch: 820, train loss, test loss , accuracy |0.34461499233824944 , 0.2638611616156243, 92.74%\n",
      "Weight has been saved !!\n",
      "epoch: 830, train loss, test loss , accuracy |0.12932198688162738 , 0.2714247988271127, 92.47999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 840, train loss, test loss , accuracy |0.2226266435498634 , 0.2636108445566964, 92.74%\n",
      "Weight has been saved !!\n",
      "epoch: 850, train loss, test loss , accuracy |0.22067693110069408 , 0.2617973715826811, 92.84%\n",
      "Weight has been saved !!\n",
      "epoch: 860, train loss, test loss , accuracy |0.33626062975897963 , 0.25629816228315355, 92.85%\n",
      "Weight has been saved !!\n",
      "epoch: 870, train loss, test loss , accuracy |0.10135857914872869 , 0.2621692583078579, 92.54%\n",
      "Weight has been saved !!\n",
      "epoch: 880, train loss, test loss , accuracy |0.3249896278692387 , 0.2534462270371798, 92.81%\n",
      "Weight has been saved !!\n",
      "epoch: 890, train loss, test loss , accuracy |0.09826855330377439 , 0.2546099070765757, 92.82000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 900, train loss, test loss , accuracy |0.22143969818536102 , 0.24668260471086614, 93.19%\n",
      "Weight has been saved !!\n",
      "epoch: 910, train loss, test loss , accuracy |0.14203855120782097 , 0.24949072048330995, 93.04%\n",
      "Weight has been saved !!\n",
      "epoch: 920, train loss, test loss , accuracy |0.18128140672411205 , 0.24976889910861724, 93.10000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 930, train loss, test loss , accuracy |0.16018854204116387 , 0.24180194252159626, 93.30000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 940, train loss, test loss , accuracy |0.2695381098079502 , 0.2449505763320314, 93.04%\n",
      "Weight has been saved !!\n",
      "epoch: 950, train loss, test loss , accuracy |0.21913311576087047 , 0.2490492116313585, 92.97%\n",
      "Weight has been saved !!\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构\n",
    "network = NLayerMLP(sizes=[784, 64, 32, 10])\n",
    "weight_save_path  = \"weight_multi_layer_adam.pickle\"\n",
    "epoches = 100000  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "iter_num = 10\n",
    "load_weight = False\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.adam(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=10,\n",
    "            save_path=weight_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3, 4]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "for i in a:\n",
    "    i = 2\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae51ee3d492f24e83e77a52eb34bf16365894f8747390aa8e17995579dedf394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
