{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functions as f\n",
    "from gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "from layers import *\n",
    "import pickle\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 定义读取数据集的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_mnist_data(kind):\n",
    "    '''\n",
    "    加载数据集\n",
    "    :param kind: 加载训练数据还是测试数据\n",
    "    :return: 打平之后的数据和one hot编码的标签\n",
    "    '''\n",
    "    labels_path = '../data/%s-labels-idx1-ubyte' % kind\n",
    "    images_path = '../data/%s-images-idx3-ubyte' % kind\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images / 255., np.eye(10)[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 定义神经网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class NLayerMLP:\n",
    "\n",
    "    def __init__(self, sizes, weight_init_method):\n",
    "        # 存储权重值和偏置值\n",
    "        self.sizes = sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for input_size, output_size in zip(sizes[0:-1], sizes[1:]):\n",
    "            # 初始化权重\n",
    "            if weight_init_method==\"0\":\n",
    "                # 使用0值初始化\n",
    "                weight = 0 * np.random.randn(input_size, output_size)\n",
    "            elif weight_init_method==\"Kaiming\":\n",
    "                # 使用Kaiming初始化\n",
    "                weight = 16*2/(input_size+output_size) * np.random.randn(input_size, output_size)\n",
    "            bias = np.zeros(output_size)\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "        # 这个神经网络的层数为n-1\n",
    "        size = len(sizes)-1\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        for i in range(0, size):\n",
    "            if i != size-1:\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "                #self.layers['Relu1'] = Relu()\n",
    "                self.layers[f'Sigmoid1{i}'] = Sigmoid()\n",
    "            else:\n",
    "                # 这里没有激活函数\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        #self.lastLayer = IdentityWithLoss()\n",
    "        self.params = dict()\n",
    "        self.params['sizes'] = self.sizes\n",
    "        self.params['biases'] = self.biases\n",
    "        self.params['weights'] = self.weights\n",
    "        self.params['layers'] = self.layers\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'] = numerical_gradient(loss_W, self.weights[i])\n",
    "            grads[f'b{i}'] = numerical_gradient(loss_W, self.biases[i])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'], grads[f'b{i}'] = self.layers[f'Affine{i}'].dW, self.layers[f'Affine{i}'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def sgd(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None,\n",
    "            loss_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        for i in range(epochs):\n",
    "            for batch_index in range(0, train_size, batch_size):\n",
    "                lower_range = batch_index\n",
    "                upper_range = batch_index + batch_size\n",
    "                if upper_range > train_size:\n",
    "                    upper_range = train_size\n",
    "                x_batch = train_features[lower_range: upper_range, :]\n",
    "                y_batch = train_labels[lower_range: upper_range]\n",
    "                # 计算梯度\n",
    "                #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "                grad = self.gradient(x_batch, y_batch)\n",
    "                # 更新参数\n",
    "                for index in range(len(network.sizes)-1):\n",
    "                    self.weights[index] -= lr * grad[f\"W{index}\"]\n",
    "                    self.biases[index] -= lr * grad[f\"b{index}\"]\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "                if loss_path:\n",
    "                    with open(loss_path, \"a\") as f:\n",
    "                        if i == 0:\n",
    "                            f.write(\"epoch train_loss test_loss  accuracy\\n\")\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "\n",
    "\n",
    "    def adam(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None,\n",
    "             loss_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        for i in range(epochs):\n",
    "            for batch_index in range(0, train_size, batch_size):\n",
    "                lower_range = batch_index\n",
    "                upper_range = batch_index + batch_size\n",
    "                if upper_range > train_size:\n",
    "                    upper_range = train_size\n",
    "                x_batch = train_features[lower_range: upper_range, :]\n",
    "                y_batch = train_labels[lower_range: upper_range]\n",
    "                # 计算梯度\n",
    "                #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "                grad = self.gradient(x_batch, y_batch)\n",
    "                # 更新参数, 这里使用adam算法\n",
    "                rho1, rho2 = 0.9, 0.999\n",
    "                delta = 1e-8\n",
    "                for index in range(len(network.sizes)-1):\n",
    "                    weight = self.weights[index]\n",
    "                    weight_grad = grad[f\"W{index}\"]\n",
    "                    s = np.zeros_like(weight)\n",
    "                    r = np.zeros_like(weight)\n",
    "                    # update the first and the second moment\n",
    "                    s = rho1 * s + (1 - rho1) * weight_grad\n",
    "                    r = rho2 * r + (1 - rho2) * np.square(weight_grad)\n",
    "                    # get the partial first and second moment\n",
    "                    s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                    r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                    # use the partial first and second moment to modify the GD method\n",
    "                    self.weights[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "                    bias = self.biases[index]\n",
    "                    bias_grad = grad[f\"b{index}\"]\n",
    "                    s = np.zeros_like(bias)\n",
    "                    r = np.zeros_like(bias)\n",
    "                    # update the first and the second moment\n",
    "                    s = rho1 * s + (1 - rho1) * bias_grad\n",
    "                    r = rho2 * r + (1 - rho2) * np.square(bias_grad)\n",
    "                    # get the partial first and second moment\n",
    "                    s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                    r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                    # use the partial first and second moment to modify the GD method\n",
    "                    self.biases[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "                if loss_path:\n",
    "                    with open(loss_path, \"a\") as f:\n",
    "                        if i == 0:\n",
    "                            f.write(\"epoch train_loss test_loss  accuracy\\n\")\n",
    "                            f.write(f\"{i}  {loss}  {loss_test} {accuracy}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{i}  {loss} {loss_test} {accuracy}\\n\")\n",
    "\n",
    "    # 保存权重文件\n",
    "    def save(self,path):\n",
    "        with open(path,'wb') as f :\n",
    "            self.params['sizes'] = self.sizes\n",
    "            self.params['biases'] = self.biases\n",
    "            self.params['weights'] = self.weights\n",
    "            self.params['layers'] = self.layers\n",
    "            pickle.dump(self.params, f)\n",
    "\n",
    "    # 导入权重文件\n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f :\n",
    "            self.params = pickle.load(f)\n",
    "            self.sizes = self.params['sizes']\n",
    "            self.biases = self.params['biases']\n",
    "            self.weights = self.params['weights']\n",
    "            self.layers = self.params['layers']\n",
    "\n",
    "\n",
    "        #print(weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 导入训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = load_mnist_data(kind='train')\n",
    "train_images = np.reshape(train_images, newshape=(-1, 1, 784))\n",
    "test_images, test_labels = load_mnist_data('t10k')\n",
    "test_images = np.reshape(test_images, newshape=(-1, 1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set_size = len(train_images)\n",
    "testing_set_size = len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 95.69%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |0.2064751866656389 , 0.18829753377274142, 95.57%\n",
      "Weight has been saved !!\n",
      "epoch: 1, train loss, test loss , accuracy |0.20866766375115844 , 0.18801436245030517, 95.62%\n",
      "Weight has been saved !!\n",
      "epoch: 2, train loss, test loss , accuracy |0.20935990598882162 , 0.18808573095869793, 95.65%\n",
      "Weight has been saved !!\n",
      "epoch: 3, train loss, test loss , accuracy |0.20933521848645575 , 0.1881304924603436, 95.65%\n",
      "Weight has been saved !!\n",
      "epoch: 4, train loss, test loss , accuracy |0.2091899965339419 , 0.18840798472866396, 95.69%\n",
      "Weight has been saved !!\n",
      "epoch: 5, train loss, test loss , accuracy |0.2090098357010374 , 0.18879363092310564, 95.67999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 6, train loss, test loss , accuracy |0.20881909358196996 , 0.18903879330508166, 95.66%\n",
      "Weight has been saved !!\n",
      "epoch: 7, train loss, test loss , accuracy |0.20864400239084865 , 0.18932176496038428, 95.67999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 8, train loss, test loss , accuracy |0.20917777889649283 , 0.18959364970653322, 95.73%\n",
      "Weight has been saved !!\n",
      "epoch: 9, train loss, test loss , accuracy |0.20974116394295417 , 0.1897270823061066, 95.74000000000001%\n",
      "Weight has been saved !!\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构\n",
    "network = NLayerMLP(sizes=[784, 64, 32, 10])\n",
    "weight_save_path  = \"weight_multi_layer_adam.pickle\"\n",
    "epoches = 10  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "iter_num = 10\n",
    "load_weight = True\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.adam(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=1,\n",
    "            save_path=weight_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 验证权重和偏置都为0时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 661, train loss, test loss , accuracy |1.6799289539170992 , 1.5306884115579271, 38.75%\n",
      "Weight has been saved !!\n",
      "epoch: 662, train loss, test loss , accuracy |1.6799115628929446 , 1.53066593287219, 38.75%\n",
      "Weight has been saved !!\n",
      "epoch: 663, train loss, test loss , accuracy |1.6798941480787288 , 1.530643524834076, 38.75%\n",
      "Weight has been saved !!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [52]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch: , train loss, test loss , accuracy |, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight has been saved !!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msgd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m             \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtest_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miter_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43miter_num\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m             \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_save_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msingle_layer_numpy_sgd.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.sgd\u001B[1;34m(self, training_data, epochs, batch_size, lr, test_data, iter_num, save_path, loss_path)\u001B[0m\n\u001B[0;32m     99\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m train_labels[lower_range: upper_range]\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# 计算梯度\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m#grad = self.numerical_gradient(x_batch, y_batch)\u001B[39;00m\n\u001B[1;32m--> 102\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# 更新参数\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(network\u001B[38;5;241m.\u001B[39msizes)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.gradient\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgradient\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# backward\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     dout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.loss\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloss\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[1;32m---> 42\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlastLayer\u001B[38;5;241m.\u001B[39mforward(y, t)\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.predict\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m---> 36\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mE:\\桌面\\深度学习数学基础\\DeepLearning-Basics\\MNIST识别实现\\Mnist MLP\\layers.py:56\u001B[0m, in \u001B[0;36mAffine.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     53\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m---> 56\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构\n",
    "network = NLayerMLP(sizes=[784, 128, 10])\n",
    "weight_save_path  = \"weight_64_hidden.pickle\"\n",
    "epoches = 10000  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-2\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "iter_num = 1\n",
    "load_weight = False\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.sgd(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "             lr=learning_rate, test_data=(test_images, test_labels), iter_num=iter_num,\n",
    "             save_path=weight_save_path, loss_path=\"single_layer_numpy_sgd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 20.75%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |2.1283100007268407 , 2.076106760557496, 20.75%\n",
      "Weight has been saved !!\n",
      "epoch: 1, train loss, test loss , accuracy |2.1282113165164653 , 2.076112322932562, 20.75%\n",
      "Weight has been saved !!\n",
      "epoch: 2, train loss, test loss , accuracy |2.1281253676766254 , 2.0761171416420927, 20.74%\n",
      "Weight has been saved !!\n",
      "epoch: 3, train loss, test loss , accuracy |2.1280514256212837 , 2.0761211863424385, 20.74%\n",
      "Weight has been saved !!\n",
      "epoch: 4, train loss, test loss , accuracy |2.127988800356441 , 2.0761244284869242, 20.74%\n",
      "Weight has been saved !!\n",
      "epoch: 5, train loss, test loss , accuracy |2.1279368388675697 , 2.076126840473701, 20.73%\n",
      "Weight has been saved !!\n",
      "epoch: 6, train loss, test loss , accuracy |2.1278949235170113 , 2.07612839490973, 20.73%\n",
      "Weight has been saved !!\n",
      "epoch: 7, train loss, test loss , accuracy |2.1278624704614657 , 2.0761290640082795, 20.73%\n",
      "Weight has been saved !!\n",
      "epoch: 8, train loss, test loss , accuracy |2.127838928092426 , 2.0761288191388045, 20.73%\n",
      "Weight has been saved !!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [49]\u001B[0m, in \u001B[0;36m<cell line: 19>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch: , train loss, test loss , accuracy |, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight has been saved !!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 19\u001B[0m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msgd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtest_images\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miter_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43miter_num\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m            \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_save_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msingle_layer_numpy_adam4.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.sgd\u001B[1;34m(self, training_data, epochs, batch_size, lr, test_data, iter_num, save_path, loss_path)\u001B[0m\n\u001B[0;32m     99\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m train_labels[lower_range: upper_range]\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# 计算梯度\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m#grad = self.numerical_gradient(x_batch, y_batch)\u001B[39;00m\n\u001B[1;32m--> 102\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# 更新参数\u001B[39;00m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(network\u001B[38;5;241m.\u001B[39msizes)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.gradient\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgradient\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m---> 66\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# backward\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     dout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.loss\u001B[1;34m(self, x, t)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloss\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[1;32m---> 42\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlastLayer\u001B[38;5;241m.\u001B[39mforward(y, t)\n",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36mNLayerMLP.predict\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m---> 36\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mE:\\桌面\\深度学习数学基础\\DeepLearning-Basics\\MNIST识别实现\\Mnist MLP\\layers.py:56\u001B[0m, in \u001B[0;36mAffine.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     53\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m---> 56\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构\n",
    "network = NLayerMLP(sizes=[784, 32,4, 10])\n",
    "weight_save_path  = \"weight_64_hidden.pickle\"\n",
    "epoches = 100000  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "iter_num = 1\n",
    "load_weight = True\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.sgd(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=iter_num,\n",
    "            save_path=weight_save_path, loss_path=\"single_layer_numpy_adam4.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 这一结用于研究不同的初始化对于最后训练得到的学习曲线的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0权重初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 9.8%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |2.154921193182849 , 2.1362275203968646, 10.54%\n",
      "Weight has been saved !!\n",
      "epoch: 10, train loss, test loss , accuracy |1.7680076426267544 , 1.7618344730282636, 22.400000000000002%\n",
      "Weight has been saved !!\n",
      "epoch: 20, train loss, test loss , accuracy |1.636665539048008 , 1.7196296295729327, 27.68%\n",
      "Weight has been saved !!\n",
      "epoch: 30, train loss, test loss , accuracy |1.6126512672328939 , 1.6387339668939715, 35.96%\n",
      "Weight has been saved !!\n",
      "epoch: 40, train loss, test loss , accuracy |1.5688362140343106 , 1.5916505507499537, 38.519999999999996%\n",
      "Weight has been saved !!\n",
      "epoch: 50, train loss, test loss , accuracy |1.535897347640858 , 1.570485058727916, 39.97%\n",
      "Weight has been saved !!\n",
      "epoch: 60, train loss, test loss , accuracy |1.5084495467610761 , 1.5618569318786477, 40.92%\n",
      "Weight has been saved !!\n",
      "epoch: 70, train loss, test loss , accuracy |1.4915745013996553 , 1.5571132590468106, 41.86%\n",
      "Weight has been saved !!\n",
      "epoch: 80, train loss, test loss , accuracy |1.4803853251855887 , 1.5546839758429944, 42.449999999999996%\n",
      "Weight has been saved !!\n",
      "epoch: 90, train loss, test loss , accuracy |1.4726126116806075 , 1.554019922490731, 42.699999999999996%\n",
      "Weight has been saved !!\n",
      "epoch: 100, train loss, test loss , accuracy |1.4679735113404486 , 1.5552643356911031, 42.79%\n",
      "Weight has been saved !!\n",
      "epoch: 110, train loss, test loss , accuracy |1.486429371206581 , 1.5578301153537017, 42.309999999999995%\n",
      "Weight has been saved !!\n",
      "epoch: 120, train loss, test loss , accuracy |1.494663903881728 , 1.5602304594663434, 41.52%\n",
      "Weight has been saved !!\n",
      "epoch: 130, train loss, test loss , accuracy |1.4974941973257703 , 1.5603681929047999, 41.199999999999996%\n",
      "Weight has been saved !!\n",
      "epoch: 140, train loss, test loss , accuracy |1.4990811034535447 , 1.561335820630411, 40.72%\n",
      "Weight has been saved !!\n",
      "epoch: 150, train loss, test loss , accuracy |1.500152811416008 , 1.5616198294208168, 40.400000000000006%\n",
      "Weight has been saved !!\n",
      "epoch: 160, train loss, test loss , accuracy |1.499707349524205 , 1.56228203214928, 40.0%\n",
      "Weight has been saved !!\n",
      "epoch: 170, train loss, test loss , accuracy |1.502201896970634 , 1.5636414047989873, 39.79%\n",
      "Weight has been saved !!\n",
      "epoch: 180, train loss, test loss , accuracy |1.500407245297259 , 1.5632698470643882, 39.81%\n",
      "Weight has been saved !!\n",
      "epoch: 190, train loss, test loss , accuracy |1.49167099933866 , 1.563263325272498, 39.739999999999995%\n",
      "Weight has been saved !!\n",
      "epoch: 200, train loss, test loss , accuracy |1.4885140845088498 , 1.563873286813451, 39.489999999999995%\n",
      "Weight has been saved !!\n",
      "epoch: 210, train loss, test loss , accuracy |1.4804903531790856 , 1.5625496922553608, 39.379999999999995%\n",
      "Weight has been saved !!\n",
      "epoch: 220, train loss, test loss , accuracy |1.4723352897908548 , 1.5617327949997808, 39.08%\n",
      "Weight has been saved !!\n",
      "epoch: 230, train loss, test loss , accuracy |1.4697412027376784 , 1.5630280110615788, 38.41%\n",
      "Weight has been saved !!\n",
      "epoch: 240, train loss, test loss , accuracy |1.467804471377815 , 1.5623771770562958, 38.17%\n",
      "Weight has been saved !!\n",
      "epoch: 250, train loss, test loss , accuracy |1.472529504725878 , 1.5617311211689013, 38.56%\n",
      "Weight has been saved !!\n",
      "epoch: 260, train loss, test loss , accuracy |1.4735139813131422 , 1.5611302147194914, 38.550000000000004%\n",
      "Weight has been saved !!\n",
      "epoch: 270, train loss, test loss , accuracy |1.4715608947547534 , 1.56059121999114, 39.2%\n",
      "Weight has been saved !!\n",
      "epoch: 280, train loss, test loss , accuracy |1.4735503352141015 , 1.5615558003892984, 39.190000000000005%\n",
      "Weight has been saved !!\n",
      "epoch: 290, train loss, test loss , accuracy |1.4798672727693194 , 1.5620590264925522, 39.04%\n",
      "Weight has been saved !!\n",
      "epoch: 300, train loss, test loss , accuracy |1.4879118162126332 , 1.56276559848089, 38.99%\n",
      "Weight has been saved !!\n",
      "epoch: 310, train loss, test loss , accuracy |1.4916526150707154 , 1.563345340003978, 38.9%\n",
      "Weight has been saved !!\n",
      "epoch: 320, train loss, test loss , accuracy |1.497732917693133 , 1.5642630044659083, 38.75%\n",
      "Weight has been saved !!\n",
      "epoch: 330, train loss, test loss , accuracy |1.4992510706986188 , 1.564499361562532, 38.89%\n",
      "Weight has been saved !!\n",
      "epoch: 340, train loss, test loss , accuracy |1.4999598869024873 , 1.5645872341197624, 38.93%\n",
      "Weight has been saved !!\n",
      "epoch: 350, train loss, test loss , accuracy |1.5064066170118349 , 1.5642759347080541, 39.14%\n",
      "Weight has been saved !!\n",
      "epoch: 360, train loss, test loss , accuracy |1.5083619615016506 , 1.5642640099878877, 39.269999999999996%\n",
      "Weight has been saved !!\n",
      "epoch: 370, train loss, test loss , accuracy |1.51010521570272 , 1.565431852456174, 39.09%\n",
      "Weight has been saved !!\n",
      "epoch: 380, train loss, test loss , accuracy |1.5052142701363274 , 1.5661329369066048, 39.25%\n",
      "Weight has been saved !!\n",
      "epoch: 390, train loss, test loss , accuracy |1.5062251081476463 , 1.566585134067288, 39.24%\n",
      "Weight has been saved !!\n",
      "epoch: 400, train loss, test loss , accuracy |1.5129550612579898 , 1.5680183769212857, 38.9%\n",
      "Weight has been saved !!\n",
      "epoch: 410, train loss, test loss , accuracy |1.5157469500134961 , 1.5696628968912751, 38.7%\n",
      "Weight has been saved !!\n",
      "epoch: 420, train loss, test loss , accuracy |1.519222653975156 , 1.569725802391334, 38.48%\n",
      "Weight has been saved !!\n",
      "epoch: 430, train loss, test loss , accuracy |1.5219588259504129 , 1.571641845467691, 38.29%\n",
      "Weight has been saved !!\n",
      "epoch: 440, train loss, test loss , accuracy |1.523275057839102 , 1.5711928194013247, 39.08%\n",
      "Weight has been saved !!\n",
      "epoch: 450, train loss, test loss , accuracy |1.5244545168501453 , 1.5714607180618376, 38.91%\n",
      "Weight has been saved !!\n",
      "epoch: 460, train loss, test loss , accuracy |1.5273592518927182 , 1.5729177664858967, 38.45%\n",
      "Weight has been saved !!\n",
      "epoch: 470, train loss, test loss , accuracy |1.5204807822577795 , 1.5742119455625205, 38.279999999999994%\n",
      "Weight has been saved !!\n",
      "epoch: 480, train loss, test loss , accuracy |1.5085399199407135 , 1.5743628028848289, 38.73%\n",
      "Weight has been saved !!\n",
      "epoch: 490, train loss, test loss , accuracy |1.4987575699175888 , 1.5748184853760112, 39.03%\n",
      "Weight has been saved !!\n",
      "epoch: 500, train loss, test loss , accuracy |1.4962530263088418 , 1.575987644870923, 38.07%\n",
      "Weight has been saved !!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32me:\\桌面\\深度学习数学基础\\DeepLearning-Basics\\MNIST识别实现\\Mnist MLP\\手写数据集MLP训练N层 .ipynb Cell 16\u001B[0m in \u001B[0;36m<cell line: 17>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001B[0m     \u001B[39mprint\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mepoch: , train loss, test loss , accuracy |, \u001B[39m\u001B[39m{\u001B[39;00maccuracy\u001B[39m*\u001B[39m\u001B[39m100\u001B[39m\u001B[39m}\u001B[39;00m\u001B[39m%\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001B[0m     \u001B[39mprint\u001B[39m(\u001B[39m\"\u001B[39m\u001B[39mWeight has been saved !!\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m---> <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001B[0m network\u001B[39m.\u001B[39;49madam(training_data\u001B[39m=\u001B[39;49m(train_images, train_labels), epochs\u001B[39m=\u001B[39;49mepoches, batch_size\u001B[39m=\u001B[39;49mbatch_size,\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001B[0m             lr\u001B[39m=\u001B[39;49mlearning_rate, test_data\u001B[39m=\u001B[39;49m(test_images, test_labels), iter_num\u001B[39m=\u001B[39;49m\u001B[39m10\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001B[0m             save_path\u001B[39m=\u001B[39;49mweight_save_path, loss_path\u001B[39m=\u001B[39;49mloss_path)\n",
      "\u001B[1;32me:\\桌面\\深度学习数学基础\\DeepLearning-Basics\\MNIST识别实现\\Mnist MLP\\手写数据集MLP训练N层 .ipynb Cell 16\u001B[0m in \u001B[0;36mNLayerMLP.adam\u001B[1;34m(self, training_data, epochs, batch_size, lr, test_data, iter_num, save_path, loss_path)\u001B[0m\n\u001B[0;32m    <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=155'>156</a>\u001B[0m r_hat \u001B[39m=\u001B[39m r \u001B[39m/\u001B[39m (\u001B[39m1\u001B[39m \u001B[39m-\u001B[39m rho1 \u001B[39m*\u001B[39m\u001B[39m*\u001B[39m (i \u001B[39m+\u001B[39m \u001B[39m1\u001B[39m))\n\u001B[0;32m    <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=156'>157</a>\u001B[0m \u001B[39m# use the partial first and second moment to modify the GD method\u001B[39;00m\n\u001B[1;32m--> <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=157'>158</a>\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mweights[index] \u001B[39m-\u001B[39m\u001B[39m=\u001B[39m lr \u001B[39m*\u001B[39m s_hat \u001B[39m/\u001B[39m (np\u001B[39m.\u001B[39;49msqrt(r_hat) \u001B[39m+\u001B[39;49m delta)\n\u001B[0;32m    <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=158'>159</a>\u001B[0m bias \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mbiases[index]\n\u001B[0;32m    <a href='vscode-notebook-cell:/e%3A/%E6%A1%8C%E9%9D%A2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/DeepLearning-Basics/MNIST%E8%AF%86%E5%88%AB%E5%AE%9E%E7%8E%B0/Mnist%20MLP/%E6%89%8B%E5%86%99%E6%95%B0%E6%8D%AE%E9%9B%86MLP%E8%AE%AD%E7%BB%83N%E5%B1%82%20.ipynb#X22sZmlsZQ%3D%3D?line=159'>160</a>\u001B[0m bias_grad \u001B[39m=\u001B[39m grad[\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mb\u001B[39m\u001B[39m{\u001B[39;00mindex\u001B[39m}\u001B[39;00m\u001B[39m\"\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构\n",
    "network = NLayerMLP(sizes=[784, 64, 32, 10], weight_init_method=\"0\")\n",
    "weight_save_path  = \"weight_multi_layer_adam_0bias.pickle\"\n",
    "loss_path = \"weight_multi_layer_adam_0bias.txt\"\n",
    "epoches = 500 # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "load_weight = False\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.adam(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=10,\n",
    "            save_path=weight_save_path, loss_path=loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面研究使用kaiming初始化的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 11.35%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |0.3154867903619187 , 0.33321272605502655, 90.38000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 10, train loss, test loss , accuracy |0.23565516152895985 , 0.18183001901991053, 94.53%\n",
      "Weight has been saved !!\n",
      "epoch: 20, train loss, test loss , accuracy |0.23938485813044816 , 0.15948160241686202, 95.19999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 30, train loss, test loss , accuracy |0.23993347796150663 , 0.15057994938864555, 95.53%\n",
      "Weight has been saved !!\n",
      "epoch: 40, train loss, test loss , accuracy |0.23539109279314452 , 0.14583404336679237, 95.82000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 50, train loss, test loss , accuracy |0.22438219965303996 , 0.14363710572699034, 96.0%\n",
      "Weight has been saved !!\n",
      "epoch: 60, train loss, test loss , accuracy |0.2175692122833194 , 0.14449644705738077, 96.03%\n",
      "Weight has been saved !!\n",
      "epoch: 70, train loss, test loss , accuracy |0.20945173083677873 , 0.14649209019102005, 96.04%\n",
      "Weight has been saved !!\n",
      "epoch: 80, train loss, test loss , accuracy |0.1993593261832051 , 0.1483892480003476, 96.11%\n",
      "Weight has been saved !!\n",
      "epoch: 90, train loss, test loss , accuracy |0.18857758943282854 , 0.15015613837703665, 96.05%\n",
      "Weight has been saved !!\n",
      "epoch: 100, train loss, test loss , accuracy |0.17924377981138573 , 0.15219765213073788, 96.25%\n",
      "Weight has been saved !!\n",
      "epoch: 110, train loss, test loss , accuracy |0.17464648501369018 , 0.1548222605879058, 96.22%\n",
      "Weight has been saved !!\n",
      "epoch: 120, train loss, test loss , accuracy |0.17534592736126775 , 0.15642514111074132, 96.19%\n",
      "Weight has been saved !!\n",
      "epoch: 130, train loss, test loss , accuracy |0.17402448526496508 , 0.15832380438641877, 96.21%\n",
      "Weight has been saved !!\n",
      "epoch: 140, train loss, test loss , accuracy |0.17307541798087353 , 0.159098851733153, 96.22%\n",
      "Weight has been saved !!\n",
      "epoch: 150, train loss, test loss , accuracy |0.17393506756623725 , 0.1607478763992296, 96.31%\n",
      "Weight has been saved !!\n",
      "epoch: 160, train loss, test loss , accuracy |0.17255249142169668 , 0.16225536766415377, 96.34%\n",
      "Weight has been saved !!\n",
      "epoch: 170, train loss, test loss , accuracy |0.17319904212729462 , 0.16383740567546845, 96.39%\n",
      "Weight has been saved !!\n",
      "epoch: 180, train loss, test loss , accuracy |0.1763340911326762 , 0.16619754569439935, 96.48%\n",
      "Weight has been saved !!\n",
      "epoch: 190, train loss, test loss , accuracy |0.1805853858379695 , 0.17007082372781032, 96.49%\n",
      "Weight has been saved !!\n",
      "epoch: 200, train loss, test loss , accuracy |0.1827758373109014 , 0.17294621495802548, 96.45%\n",
      "Weight has been saved !!\n",
      "epoch: 210, train loss, test loss , accuracy |0.18196247837445745 , 0.17461871081492844, 96.33%\n",
      "Weight has been saved !!\n",
      "epoch: 220, train loss, test loss , accuracy |0.18633158982710538 , 0.1779752248262612, 96.32%\n",
      "Weight has been saved !!\n",
      "epoch: 230, train loss, test loss , accuracy |0.1867856069677544 , 0.18378157017523458, 96.22%\n",
      "Weight has been saved !!\n",
      "epoch: 240, train loss, test loss , accuracy |0.18892390936876802 , 0.18641303597620812, 96.28999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 250, train loss, test loss , accuracy |0.19189304835531942 , 0.1889992944155986, 96.39999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 260, train loss, test loss , accuracy |0.1908388205358351 , 0.19013908595674084, 96.36%\n",
      "Weight has been saved !!\n",
      "epoch: 270, train loss, test loss , accuracy |0.193963338429798 , 0.18960341875124323, 96.45%\n",
      "Weight has been saved !!\n",
      "epoch: 280, train loss, test loss , accuracy |0.20020577078781562 , 0.18863561466126072, 96.43%\n",
      "Weight has been saved !!\n",
      "epoch: 290, train loss, test loss , accuracy |0.20168798626561957 , 0.18999198036946055, 96.37%\n",
      "Weight has been saved !!\n",
      "epoch: 300, train loss, test loss , accuracy |0.1986626998780275 , 0.1943024508675406, 96.45%\n",
      "Weight has been saved !!\n",
      "epoch: 310, train loss, test loss , accuracy |0.20480124599609636 , 0.20164446940019953, 96.28999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 320, train loss, test loss , accuracy |0.2077866210715195 , 0.20718817085706223, 96.37%\n",
      "Weight has been saved !!\n",
      "epoch: 330, train loss, test loss , accuracy |0.22147331844010243 , 0.2141108431365719, 96.31%\n",
      "Weight has been saved !!\n",
      "epoch: 340, train loss, test loss , accuracy |0.23014847746732758 , 0.22121179730383164, 96.26%\n",
      "Weight has been saved !!\n",
      "epoch: 350, train loss, test loss , accuracy |0.22434490941991714 , 0.22755352823790198, 96.19%\n",
      "Weight has been saved !!\n",
      "epoch: 360, train loss, test loss , accuracy |0.2148007439340849 , 0.2344206230234871, 96.11%\n",
      "Weight has been saved !!\n",
      "epoch: 370, train loss, test loss , accuracy |0.2121850933917977 , 0.24090536766672416, 95.99%\n",
      "Weight has been saved !!\n",
      "epoch: 380, train loss, test loss , accuracy |0.21063466879542714 , 0.24656174996549138, 96.00999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 390, train loss, test loss , accuracy |0.21832527387659373 , 0.2519641610356716, 96.04%\n",
      "Weight has been saved !!\n",
      "epoch: 400, train loss, test loss , accuracy |0.22359000434989776 , 0.25645343607119914, 96.1%\n",
      "Weight has been saved !!\n",
      "epoch: 410, train loss, test loss , accuracy |0.22178261917371622 , 0.25780219507575297, 96.11%\n",
      "Weight has been saved !!\n",
      "epoch: 420, train loss, test loss , accuracy |0.22695741797514743 , 0.25835181378987115, 96.19%\n",
      "Weight has been saved !!\n",
      "epoch: 430, train loss, test loss , accuracy |0.23763609725747648 , 0.26455826853988634, 96.02000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 440, train loss, test loss , accuracy |0.23128932872371175 , 0.2697078788992843, 96.08%\n",
      "Weight has been saved !!\n",
      "epoch: 450, train loss, test loss , accuracy |0.20678194573678202 , 0.2739409994604262, 96.03%\n",
      "Weight has been saved !!\n",
      "epoch: 460, train loss, test loss , accuracy |0.18650968715519403 , 0.27811317933360535, 95.99%\n",
      "Weight has been saved !!\n",
      "epoch: 470, train loss, test loss , accuracy |0.17777692631027697 , 0.28328758629738265, 95.98%\n",
      "Weight has been saved !!\n",
      "epoch: 480, train loss, test loss , accuracy |0.1744325425660547 , 0.2886871000947931, 96.03%\n",
      "Weight has been saved !!\n",
      "epoch: 490, train loss, test loss , accuracy |0.1755819255818424 , 0.29388222224074295, 95.89999999999999%\n",
      "Weight has been saved !!\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构,使用kaiming初始化\n",
    "network = NLayerMLP(sizes=[784, 64, 32, 10], weight_init_method=\"Kaiming\")\n",
    "weight_save_path  = \"weight_multi_layer_adam_Kaiming.pickle\"\n",
    "loss_path = \"weight_multi_layer_adam_Kaiming.txt\"\n",
    "epoches = 500  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "load_weight = False\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.adam(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=10,\n",
    "            save_path=weight_save_path, loss_path=loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 研究学习率调整策略对于输出的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 10.280000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |0.36999324787357185 , 0.34525710337647275, 90.47%\n",
      "Weight has been saved !!\n",
      "epoch: 1, train loss, test loss , accuracy |0.3108812395775607 , 0.28147270278859315, 91.75999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 2, train loss, test loss , accuracy |0.2864337502089635 , 0.25483394205216703, 92.45%\n",
      "Weight has been saved !!\n",
      "epoch: 3, train loss, test loss , accuracy |0.27278712678115696 , 0.23779430631398304, 92.99%\n",
      "Weight has been saved !!\n",
      "epoch: 4, train loss, test loss , accuracy |0.2640487231560845 , 0.2252342962847426, 93.21000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 5, train loss, test loss , accuracy |0.2564452196223177 , 0.21527052864182072, 93.45%\n",
      "Weight has been saved !!\n",
      "epoch: 6, train loss, test loss , accuracy |0.24927962358163122 , 0.20705110328986628, 93.73%\n",
      "Weight has been saved !!\n",
      "epoch: 7, train loss, test loss , accuracy |0.24257834300683045 , 0.1998936044195117, 93.91000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 8, train loss, test loss , accuracy |0.2362788409966452 , 0.19366563772426343, 94.11%\n",
      "Weight has been saved !!\n",
      "epoch: 9, train loss, test loss , accuracy |0.23076481895932432 , 0.18816996504565442, 94.32000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 10, train loss, test loss , accuracy |0.22617253021895367 , 0.18330820360280997, 94.47%\n",
      "Weight has been saved !!\n",
      "epoch: 11, train loss, test loss , accuracy |0.22226034458771737 , 0.1790031136638421, 94.57%\n",
      "Weight has been saved !!\n",
      "epoch: 12, train loss, test loss , accuracy |0.21909548392066222 , 0.17513018086393475, 94.69999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 13, train loss, test loss , accuracy |0.21665419931670682 , 0.17161015519851053, 94.87%\n",
      "Weight has been saved !!\n",
      "epoch: 14, train loss, test loss , accuracy |0.21494110560725935 , 0.16842807271816407, 95.02000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 15, train loss, test loss , accuracy |0.21370458286110902 , 0.1655421023473616, 95.17%\n",
      "Weight has been saved !!\n",
      "epoch: 16, train loss, test loss , accuracy |0.21315400535516063 , 0.16283849262659397, 95.22%\n",
      "Weight has been saved !!\n",
      "epoch: 17, train loss, test loss , accuracy |0.2128301936560638 , 0.160370326139095, 95.3%\n",
      "Weight has been saved !!\n",
      "epoch: 18, train loss, test loss , accuracy |0.21315848495111708 , 0.15802454384202158, 95.35%\n",
      "Weight has been saved !!\n",
      "epoch: 19, train loss, test loss , accuracy |0.21338740740909093 , 0.1558903666896787, 95.45%\n",
      "Weight has been saved !!\n",
      "epoch: 20, train loss, test loss , accuracy |0.21385036639559685 , 0.1539361994168331, 95.5%\n",
      "Weight has been saved !!\n",
      "epoch: 21, train loss, test loss , accuracy |0.21443510552586595 , 0.1522342041209119, 95.52000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 22, train loss, test loss , accuracy |0.21518466990294835 , 0.15070930791937143, 95.57%\n",
      "Weight has been saved !!\n",
      "epoch: 23, train loss, test loss , accuracy |0.2157588981975318 , 0.14930027952849317, 95.62%\n",
      "Weight has been saved !!\n",
      "epoch: 24, train loss, test loss , accuracy |0.2159821470500376 , 0.14798343088963523, 95.67999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 25, train loss, test loss , accuracy |0.21631036477755453 , 0.1467443663466876, 95.74000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 26, train loss, test loss , accuracy |0.21644462612600937 , 0.14565448605610265, 95.78999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 27, train loss, test loss , accuracy |0.21645733486615545 , 0.14461972277348964, 95.86%\n",
      "Weight has been saved !!\n",
      "epoch: 28, train loss, test loss , accuracy |0.2166707323931876 , 0.14366771556131838, 95.94%\n",
      "Weight has been saved !!\n",
      "epoch: 29, train loss, test loss , accuracy |0.21658051889409557 , 0.14281366086378444, 96.0%\n",
      "Weight has been saved !!\n",
      "epoch: 30, train loss, test loss , accuracy |0.21658233759123968 , 0.1420605900972072, 96.00999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 31, train loss, test loss , accuracy |0.21615962317997858 , 0.14142093664375532, 96.05%\n",
      "Weight has been saved !!\n",
      "epoch: 32, train loss, test loss , accuracy |0.21592192143230834 , 0.14083492300216532, 96.05%\n",
      "Weight has been saved !!\n",
      "epoch: 33, train loss, test loss , accuracy |0.21553739306463385 , 0.14033828875456672, 96.05%\n",
      "Weight has been saved !!\n",
      "epoch: 34, train loss, test loss , accuracy |0.21490875608330987 , 0.13989685925905643, 96.02000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 35, train loss, test loss , accuracy |0.21435075721400024 , 0.13948999312347818, 96.07%\n",
      "Weight has been saved !!\n",
      "epoch: 36, train loss, test loss , accuracy |0.21343906220027012 , 0.13915736784924138, 96.06%\n",
      "Weight has been saved !!\n",
      "epoch: 37, train loss, test loss , accuracy |0.21233109531393113 , 0.13886413006118725, 96.07%\n",
      "Weight has been saved !!\n",
      "epoch: 38, train loss, test loss , accuracy |0.2111671719075341 , 0.13864740019017924, 96.05%\n",
      "Weight has been saved !!\n",
      "epoch: 39, train loss, test loss , accuracy |0.2100027076765004 , 0.13848147827008553, 96.04%\n",
      "Weight has been saved !!\n",
      "epoch: 40, train loss, test loss , accuracy |0.2088311147790498 , 0.13839259279584895, 96.04%\n",
      "Weight has been saved !!\n",
      "epoch: 41, train loss, test loss , accuracy |0.20771083985092087 , 0.13834993526745873, 96.06%\n",
      "Weight has been saved !!\n",
      "epoch: 42, train loss, test loss , accuracy |0.2063143942666764 , 0.13830926526592793, 96.09%\n",
      "Weight has been saved !!\n",
      "epoch: 43, train loss, test loss , accuracy |0.20475556790556296 , 0.13832667279042624, 96.09%\n",
      "Weight has been saved !!\n",
      "epoch: 44, train loss, test loss , accuracy |0.20306293996958835 , 0.13839497091486377, 96.12%\n",
      "Weight has been saved !!\n",
      "epoch: 45, train loss, test loss , accuracy |0.2015398649397421 , 0.13850088790404413, 96.12%\n",
      "Weight has been saved !!\n",
      "epoch: 46, train loss, test loss , accuracy |0.20031754291119683 , 0.13867203635052205, 96.16%\n",
      "Weight has been saved !!\n",
      "epoch: 47, train loss, test loss , accuracy |0.19953895426021695 , 0.13880543194645753, 96.17%\n",
      "Weight has been saved !!\n",
      "epoch: 48, train loss, test loss , accuracy |0.19929688361550835 , 0.13891964136049426, 96.2%\n",
      "Weight has been saved !!\n",
      "epoch: 49, train loss, test loss , accuracy |0.19941878940144994 , 0.13897546233135846, 96.21%\n",
      "Weight has been saved !!\n",
      "epoch: 50, train loss, test loss , accuracy |0.199340625113684 , 0.13902836626437515, 96.28%\n",
      "Weight has been saved !!\n",
      "epoch: 51, train loss, test loss , accuracy |0.19958473547084019 , 0.13906650727259667, 96.27%\n",
      "Weight has been saved !!\n",
      "epoch: 52, train loss, test loss , accuracy |0.19987277966900244 , 0.13914654826441245, 96.28999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 53, train loss, test loss , accuracy |0.20041380211412363 , 0.13918133546323988, 96.3%\n",
      "Weight has been saved !!\n",
      "epoch: 54, train loss, test loss , accuracy |0.20100350006119128 , 0.13922791542305507, 96.24000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 55, train loss, test loss , accuracy |0.20174933288562483 , 0.1392770374357412, 96.26%\n",
      "Weight has been saved !!\n",
      "epoch: 56, train loss, test loss , accuracy |0.2026438161774544 , 0.13936500809896815, 96.27%\n",
      "Weight has been saved !!\n",
      "epoch: 57, train loss, test loss , accuracy |0.20374037968991987 , 0.1394263395885397, 96.27%\n",
      "Weight has been saved !!\n",
      "epoch: 58, train loss, test loss , accuracy |0.20479704844375501 , 0.1395158000514974, 96.27%\n",
      "Weight has been saved !!\n",
      "epoch: 59, train loss, test loss , accuracy |0.20608697710704463 , 0.1395810984587939, 96.27%\n",
      "Weight has been saved !!\n",
      "epoch: 60, train loss, test loss , accuracy |0.2072525625147386 , 0.1397230295711831, 96.32%\n",
      "Weight has been saved !!\n",
      "epoch: 61, train loss, test loss , accuracy |0.20860712018639097 , 0.13993077816079763, 96.33%\n",
      "Weight has been saved !!\n",
      "epoch: 62, train loss, test loss , accuracy |0.20975415656156138 , 0.1401794416442302, 96.35000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 63, train loss, test loss , accuracy |0.2109459165581964 , 0.1404536024090381, 96.34%\n",
      "Weight has been saved !!\n",
      "epoch: 64, train loss, test loss , accuracy |0.21231457521802918 , 0.1407653680595604, 96.35000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 65, train loss, test loss , accuracy |0.21393661582322984 , 0.14106686797496248, 96.33%\n",
      "Weight has been saved !!\n",
      "epoch: 66, train loss, test loss , accuracy |0.21547689358662347 , 0.1413764378961593, 96.38%\n",
      "Weight has been saved !!\n",
      "epoch: 67, train loss, test loss , accuracy |0.2167116781936224 , 0.14165611237625633, 96.41999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 68, train loss, test loss , accuracy |0.21809231433910425 , 0.14192469603825741, 96.39999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 69, train loss, test loss , accuracy |0.2196326551403487 , 0.14218864288359745, 96.39999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 70, train loss, test loss , accuracy |0.22118391669554804 , 0.14240338873026814, 96.41%\n",
      "Weight has been saved !!\n",
      "epoch: 71, train loss, test loss , accuracy |0.22269565137290373 , 0.14266416387762196, 96.41%\n",
      "Weight has been saved !!\n",
      "epoch: 72, train loss, test loss , accuracy |0.22401213440920464 , 0.14294400637480426, 96.41%\n",
      "Weight has been saved !!\n",
      "epoch: 73, train loss, test loss , accuracy |0.22530002903823684 , 0.14321978516428566, 96.43%\n",
      "Weight has been saved !!\n",
      "epoch: 74, train loss, test loss , accuracy |0.22647409635334811 , 0.14345546879290222, 96.41%\n",
      "Weight has been saved !!\n",
      "epoch: 75, train loss, test loss , accuracy |0.22765293033533698 , 0.1437477488723282, 96.45%\n",
      "Weight has been saved !!\n",
      "epoch: 76, train loss, test loss , accuracy |0.2287363138331748 , 0.14407289065898643, 96.46000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 77, train loss, test loss , accuracy |0.2302064547421856 , 0.14441993025923852, 96.44%\n",
      "Weight has been saved !!\n",
      "epoch: 78, train loss, test loss , accuracy |0.23177197692924337 , 0.14480782556008387, 96.41%\n",
      "Weight has been saved !!\n",
      "epoch: 79, train loss, test loss , accuracy |0.23312809712552077 , 0.14517256993070468, 96.38%\n",
      "Weight has been saved !!\n",
      "epoch: 80, train loss, test loss , accuracy |0.2344276007592406 , 0.14554492585213044, 96.33%\n",
      "Weight has been saved !!\n",
      "epoch: 81, train loss, test loss , accuracy |0.2356953847602438 , 0.1459354181223535, 96.34%\n",
      "Weight has been saved !!\n",
      "epoch: 82, train loss, test loss , accuracy |0.23663713548999335 , 0.1464074867194847, 96.32%\n",
      "Weight has been saved !!\n",
      "epoch: 83, train loss, test loss , accuracy |0.23792916039099735 , 0.14690217764190147, 96.33%\n",
      "Weight has been saved !!\n",
      "epoch: 84, train loss, test loss , accuracy |0.23922596280778888 , 0.14735830651836904, 96.28999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 85, train loss, test loss , accuracy |0.2405925144243064 , 0.14778712193639043, 96.23%\n",
      "Weight has been saved !!\n",
      "epoch: 86, train loss, test loss , accuracy |0.24190501967605982 , 0.1481436054716643, 96.21%\n",
      "Weight has been saved !!\n",
      "epoch: 87, train loss, test loss , accuracy |0.24331941166580237 , 0.14847010315646902, 96.23%\n",
      "Weight has been saved !!\n",
      "epoch: 88, train loss, test loss , accuracy |0.2444118002137848 , 0.14877774040195804, 96.23%\n",
      "Weight has been saved !!\n",
      "epoch: 89, train loss, test loss , accuracy |0.24564005649144968 , 0.14908540997201686, 96.24000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 90, train loss, test loss , accuracy |0.24628765753449322 , 0.14937042300026113, 96.22%\n",
      "Weight has been saved !!\n",
      "epoch: 91, train loss, test loss , accuracy |0.2471908910894176 , 0.14962213079638123, 96.23%\n",
      "Weight has been saved !!\n",
      "epoch: 92, train loss, test loss , accuracy |0.247786344394867 , 0.14987381750792375, 96.21%\n",
      "Weight has been saved !!\n",
      "epoch: 93, train loss, test loss , accuracy |0.24837192736984137 , 0.15006361783503086, 96.26%\n",
      "Weight has been saved !!\n",
      "epoch: 94, train loss, test loss , accuracy |0.24876646810621747 , 0.1502628184038933, 96.27%\n",
      "Weight has been saved !!\n",
      "epoch: 95, train loss, test loss , accuracy |0.24898701518409835 , 0.150479427770048, 96.28999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 96, train loss, test loss , accuracy |0.2490660319087509 , 0.1506991956810607, 96.26%\n",
      "Weight has been saved !!\n",
      "epoch: 97, train loss, test loss , accuracy |0.24897178222575142 , 0.15094718309958954, 96.24000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 98, train loss, test loss , accuracy |0.24898196190877972 , 0.15117405583899104, 96.25%\n",
      "Weight has been saved !!\n",
      "epoch: 99, train loss, test loss , accuracy |0.24872501607544326 , 0.15134875066356884, 96.25%\n",
      "Weight has been saved !!\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络架构,使用kaiming初始化\n",
    "network = NLayerMLP(sizes=[784, 64, 32, 10], weight_init_method=\"Kaiming\")\n",
    "weight_save_path  = \"weight_multi_layer_adam_fixed_lr.pickle\"\n",
    "loss_path = \"weight_multi_layer_adam_fixed_lr.txt\"\n",
    "epoches = 100  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "load_weight = False\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.adam(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=1,\n",
    "            save_path=weight_save_path, loss_path=loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指数衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 9.58%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |0.19317891220512454 , 0.8017736782839289, 87.56%\n",
      "Weight has been saved !!\n",
      "epoch: 1, train loss, test loss , accuracy |0.1696701951360182 , 0.44602891171214043, 93.87%\n",
      "Weight has been saved !!\n",
      "epoch: 2, train loss, test loss , accuracy |0.16793174732309457 , 0.39880672253931093, 94.69999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 3, train loss, test loss , accuracy |0.16796456650479785 , 0.3610016610838291, 95.00999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 4, train loss, test loss , accuracy |0.1680064226485237 , 0.3539327935282079, 95.13000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 5, train loss, test loss , accuracy |0.16812004108932907 , 0.3482281300490547, 95.14%\n",
      "Weight has been saved !!\n",
      "epoch: 6, train loss, test loss , accuracy |0.1680170056098547 , 0.34886144257309354, 95.08%\n",
      "Weight has been saved !!\n",
      "epoch: 7, train loss, test loss , accuracy |0.16801859929957452 , 0.3497500155084903, 95.05%\n",
      "Weight has been saved !!\n",
      "epoch: 8, train loss, test loss , accuracy |0.1679938960345806 , 0.3509134952510186, 95.15%\n",
      "Weight has been saved !!\n",
      "epoch: 9, train loss, test loss , accuracy |0.1679759556157918 , 0.35185643054095533, 95.16%\n",
      "Weight has been saved !!\n",
      "epoch: 10, train loss, test loss , accuracy |0.16796668473991264 , 0.3525866896136491, 95.17%\n",
      "Weight has been saved !!\n",
      "epoch: 11, train loss, test loss , accuracy |0.1679616354690856 , 0.35294130111865013, 95.15%\n",
      "Weight has been saved !!\n",
      "epoch: 12, train loss, test loss , accuracy |0.16795921152252835 , 0.35312159713776364, 95.17%\n",
      "Weight has been saved !!\n",
      "epoch: 13, train loss, test loss , accuracy |0.16795832752099826 , 0.35321884261158154, 95.17999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 14, train loss, test loss , accuracy |0.1679577587997705 , 0.3532782080104138, 95.19%\n",
      "Weight has been saved !!\n",
      "epoch: 15, train loss, test loss , accuracy |0.16795752749046577 , 0.353310528893606, 95.19%\n",
      "Weight has been saved !!\n",
      "epoch: 16, train loss, test loss , accuracy |0.1679574041822847 , 0.3533302043050982, 95.21%\n",
      "Weight has been saved !!\n",
      "epoch: 17, train loss, test loss , accuracy |0.16795733084294362 , 0.35334197835315695, 95.21%\n",
      "Weight has been saved !!\n",
      "epoch: 18, train loss, test loss , accuracy |0.16795728753753617 , 0.3533492979449341, 95.22%\n",
      "Weight has been saved !!\n",
      "epoch: 19, train loss, test loss , accuracy |0.16795726080613113 , 0.3533538622365297, 95.22%\n",
      "Weight has been saved !!\n",
      "epoch: 20, train loss, test loss , accuracy |0.16795724452116068 , 0.35335671466441626, 95.22%\n",
      "Weight has been saved !!\n",
      "epoch: 21, train loss, test loss , accuracy |0.16795723450881672 , 0.35335846701104834, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 22, train loss, test loss , accuracy |0.16795722831785295 , 0.3533595532101742, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 23, train loss, test loss , accuracy |0.16795722447516304 , 0.35336022690414004, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 24, train loss, test loss , accuracy |0.167957222082299 , 0.353360644950861, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 25, train loss, test loss , accuracy |0.1679572205900833 , 0.3533609043370162, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 26, train loss, test loss , accuracy |0.16795721965866997 , 0.35336106534449935, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 27, train loss, test loss , accuracy |0.16795721907782904 , 0.35336116536243445, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 28, train loss, test loss , accuracy |0.167957218715611 , 0.35336122754375476, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 29, train loss, test loss , accuracy |0.16795721848968695 , 0.35336126622957703, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 30, train loss, test loss , accuracy |0.16795721834874122 , 0.35336129031106517, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 31, train loss, test loss , accuracy |0.16795721826079016 , 0.353361305308285, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 32, train loss, test loss , accuracy |0.16795721820589612 , 0.3533613146521206, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 33, train loss, test loss , accuracy |0.1679572181716272 , 0.3533613204759737, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 34, train loss, test loss , accuracy |0.16795721815023004 , 0.3533613241071718, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 35, train loss, test loss , accuracy |0.16795721813686762 , 0.35336132637196455, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 36, train loss, test loss , accuracy |0.16795721812852146 , 0.353361327784931, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 37, train loss, test loss , accuracy |0.16795721812330786 , 0.3533613286666853, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 38, train loss, test loss , accuracy |0.1679572181200505 , 0.35336132921706725, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 39, train loss, test loss , accuracy |0.16795721811801514 , 0.35336132956068145, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 40, train loss, test loss , accuracy |0.16795721811674327 , 0.3533613297752469, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 41, train loss, test loss , accuracy |0.16795721811594844 , 0.35336132990925206, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 42, train loss, test loss , accuracy |0.1679572181154517 , 0.35336132999295655, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 43, train loss, test loss , accuracy |0.1679572181151411 , 0.3533613300452488, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 44, train loss, test loss , accuracy |0.16795721811494702 , 0.3533613300779205, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 45, train loss, test loss , accuracy |0.1679572181148257 , 0.35336133009833604, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 46, train loss, test loss , accuracy |0.16795721811474984 , 0.3533613301110942, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 47, train loss, test loss , accuracy |0.16795721811470243 , 0.35336133011906784, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 48, train loss, test loss , accuracy |0.16795721811467276 , 0.3533613301240518, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 49, train loss, test loss , accuracy |0.16795721811465417 , 0.35336133012716703, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 50, train loss, test loss , accuracy |0.1679572181146426 , 0.3533613301291145, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 51, train loss, test loss , accuracy |0.16795721811463535 , 0.3533613301303321, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 52, train loss, test loss , accuracy |0.16795721811463082 , 0.3533613301310934, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 53, train loss, test loss , accuracy |0.16795721811462797 , 0.353361330131569, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 54, train loss, test loss , accuracy |0.16795721811462624 , 0.3533613301318664, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 55, train loss, test loss , accuracy |0.16795721811462513 , 0.35336133013205234, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 56, train loss, test loss , accuracy |0.1679572181146244 , 0.35336133013216825, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 57, train loss, test loss , accuracy |0.16795721811462397 , 0.35336133013224075, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 58, train loss, test loss , accuracy |0.16795721811462372 , 0.35336133013228593, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 59, train loss, test loss , accuracy |0.16795721811462352 , 0.35336133013231413, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 60, train loss, test loss , accuracy |0.1679572181146234 , 0.3533613301323317, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 61, train loss, test loss , accuracy |0.16795721811462339 , 0.35336133013234267, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 62, train loss, test loss , accuracy |0.16795721811462336 , 0.3533613301323491, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 63, train loss, test loss , accuracy |0.1679572181146233 , 0.3533613301323531, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 64, train loss, test loss , accuracy |0.1679572181146233 , 0.3533613301323555, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 65, train loss, test loss , accuracy |0.16795721811462327 , 0.3533613301323565, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 66, train loss, test loss , accuracy |0.16795721811462327 , 0.3533613301323569, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 67, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235654, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 68, train loss, test loss , accuracy |0.16795721811462327 , 0.3533613301323563, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 69, train loss, test loss , accuracy |0.16795721811462327 , 0.3533613301323558, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 70, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235577, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 71, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235566, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 72, train loss, test loss , accuracy |0.16795721811462327 , 0.3533613301323556, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 73, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 74, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 75, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 76, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 77, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 78, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 79, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 80, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 81, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 82, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 83, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 84, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 85, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 86, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 87, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 88, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 89, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 90, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 91, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 92, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 93, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 94, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 95, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 96, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 97, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 98, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n",
      "epoch: 99, train loss, test loss , accuracy |0.16795721811462327 , 0.35336133013235554, 95.23%\n",
      "Weight has been saved !!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from math import gamma\n",
    "\n",
    "\n",
    "class NLayerMLP:\n",
    "\n",
    "    def __init__(self, sizes, weight_init_method):\n",
    "        # 存储权重值和偏置值\n",
    "        self.sizes = sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for input_size, output_size in zip(sizes[0:-1], sizes[1:]):\n",
    "            # 初始化权重\n",
    "            if weight_init_method==\"0\":\n",
    "                # 使用0值初始化\n",
    "                weight = 0 * np.random.randn(input_size, output_size)\n",
    "            elif weight_init_method==\"Kaiming\":\n",
    "                # 使用Kaiming初始化\n",
    "                weight = 16*2/(input_size+output_size) * np.random.randn(input_size, output_size)\n",
    "            bias = np.zeros(output_size)\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "        # 这个神经网络的层数为n-1\n",
    "        size = len(sizes)-1\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        for i in range(0, size):\n",
    "            if i != size-1:\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "                #self.layers['Relu1'] = Relu()\n",
    "                self.layers[f'Sigmoid1{i}'] = Sigmoid()\n",
    "            else:\n",
    "                # 这里没有激活函数\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        #self.lastLayer = IdentityWithLoss()\n",
    "        self.params = dict()\n",
    "        self.params['sizes'] = self.sizes\n",
    "        self.params['biases'] = self.biases\n",
    "        self.params['weights'] = self.weights\n",
    "        self.params['layers'] = self.layers\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'] = numerical_gradient(loss_W, self.weights[i])\n",
    "            grads[f'b{i}'] = numerical_gradient(loss_W, self.biases[i])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'], grads[f'b{i}'] = self.layers[f'Affine{i}'].dW, self.layers[f'Affine{i}'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def sgd(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None,\n",
    "            loss_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        for i in range(epochs):\n",
    "            for batch_index in range(0, train_size, batch_size):\n",
    "                lower_range = batch_index\n",
    "                upper_range = batch_index + batch_size\n",
    "                if upper_range > train_size:\n",
    "                    upper_range = train_size\n",
    "                x_batch = train_features[lower_range: upper_range, :]\n",
    "                y_batch = train_labels[lower_range: upper_range]\n",
    "                # 计算梯度\n",
    "                #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "                grad = self.gradient(x_batch, y_batch)\n",
    "                # 更新参数\n",
    "                for index in range(len(network.sizes)-1):\n",
    "                    self.weights[index] -= lr * grad[f\"W{index}\"]\n",
    "                    self.biases[index] -= lr * grad[f\"b{index}\"]\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "                if loss_path:\n",
    "                    with open(loss_path, \"a\") as f:\n",
    "                        if i == 0:\n",
    "                            f.write(\"epoch train_loss test_loss  accuracy\\n\")\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "\n",
    "\n",
    "    def adam(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None,\n",
    "             loss_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        gamma = 0.999\n",
    "        for i in range(epochs):\n",
    "            for batch_index in range(0, train_size, batch_size):\n",
    "                lower_range = batch_index\n",
    "                upper_range = batch_index + batch_size\n",
    "                if upper_range > train_size:\n",
    "                    upper_range = train_size\n",
    "                x_batch = train_features[lower_range: upper_range, :]\n",
    "                y_batch = train_labels[lower_range: upper_range]\n",
    "                # 计算梯度\n",
    "                #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "                grad = self.gradient(x_batch, y_batch)\n",
    "                # 更新参数, 这里使用adam算法\n",
    "                rho1, rho2 = 0.9, 0.999\n",
    "                delta = 1e-8\n",
    "                for index in range(len(network.sizes)-1):\n",
    "                    weight = self.weights[index]\n",
    "                    weight_grad = grad[f\"W{index}\"]\n",
    "                    s = np.zeros_like(weight)\n",
    "                    r = np.zeros_like(weight)\n",
    "                    # update the first and the second moment\n",
    "                    s = rho1 * s + (1 - rho1) * weight_grad\n",
    "                    r = rho2 * r + (1 - rho2) * np.square(weight_grad)\n",
    "                    # get the partial first and second moment\n",
    "                    s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                    r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                    # use the partial first and second moment to modify the GD method\n",
    "                    self.weights[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "                    bias = self.biases[index]\n",
    "                    bias_grad = grad[f\"b{index}\"]\n",
    "                    s = np.zeros_like(bias)\n",
    "                    r = np.zeros_like(bias)\n",
    "                    # update the first and the second moment\n",
    "                    s = rho1 * s + (1 - rho1) * bias_grad\n",
    "                    r = rho2 * r + (1 - rho2) * np.square(bias_grad)\n",
    "                    # get the partial first and second moment\n",
    "                    s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                    r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                    # use the partial first and second moment to modify the GD method\n",
    "                    self.biases[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "                lr *= gamma\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "                if loss_path:\n",
    "                    with open(loss_path, \"a\") as f:\n",
    "                        if i == 0:\n",
    "                            f.write(\"epoch train_loss test_loss  accuracy\\n\")\n",
    "                            f.write(f\"{i}  {loss}  {loss_test} {accuracy}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{i}  {loss} {loss_test} {accuracy}\\n\")\n",
    "\n",
    "    # 保存权重文件\n",
    "    def save(self,path):\n",
    "        with open(path,'wb') as f :\n",
    "            self.params['sizes'] = self.sizes\n",
    "            self.params['biases'] = self.biases\n",
    "            self.params['weights'] = self.weights\n",
    "            self.params['layers'] = self.layers\n",
    "            pickle.dump(self.params, f)\n",
    "\n",
    "    # 导入权重文件\n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f :\n",
    "            self.params = pickle.load(f)\n",
    "            self.sizes = self.params['sizes']\n",
    "            self.biases = self.params['biases']\n",
    "            self.weights = self.params['weights']\n",
    "            self.layers = self.params['layers']\n",
    "\n",
    "\n",
    "        #print(weight)\n",
    "\n",
    "# 定义神经网络架构,使用kaiming初始化\n",
    "network = NLayerMLP(sizes=[784, 64, 32, 10], weight_init_method=\"Kaiming\")\n",
    "weight_save_path  = \"weight_multi_layer_adam_e_lr.pickle\"\n",
    "loss_path = \"weight_multi_layer_adam_e_lr.txt\"\n",
    "epoches = 100  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-2\n",
    "load_weight = False\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.adam(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=1,\n",
    "            save_path=weight_save_path, loss_path=loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 阶梯衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: , train loss, test loss , accuracy |, 10.280000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 0, train loss, test loss , accuracy |0.3154399384451862 , 1.1588873630235947, 84.39999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 1, train loss, test loss , accuracy |0.17922283162186578 , 0.9391755776598456, 89.03999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 2, train loss, test loss , accuracy |0.18524941679578114 , 0.6520427899676913, 92.53%\n",
      "Weight has been saved !!\n",
      "epoch: 3, train loss, test loss , accuracy |0.2630809954264023 , 0.5947529770786859, 93.39%\n",
      "Weight has been saved !!\n",
      "epoch: 4, train loss, test loss , accuracy |0.17609907846694559 , 0.5744752285265365, 93.76%\n",
      "Weight has been saved !!\n",
      "epoch: 5, train loss, test loss , accuracy |0.20032665774703692 , 0.5320395659121544, 94.34%\n",
      "Weight has been saved !!\n",
      "epoch: 6, train loss, test loss , accuracy |0.37849415979606005 , 0.5368946255251452, 94.07%\n",
      "Weight has been saved !!\n",
      "epoch: 7, train loss, test loss , accuracy |0.2304809889065346 , 0.49229759911195153, 94.89%\n",
      "Weight has been saved !!\n",
      "epoch: 8, train loss, test loss , accuracy |0.20679417130660893 , 0.5042579071547872, 94.8%\n",
      "Weight has been saved !!\n",
      "epoch: 9, train loss, test loss , accuracy |0.16789965661551867 , 0.500919029334435, 94.71000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 10, train loss, test loss , accuracy |0.1679839325816738 , 0.5275164713479439, 94.8%\n",
      "Weight has been saved !!\n",
      "epoch: 11, train loss, test loss , accuracy |0.16793932760985572 , 0.48957025069104065, 94.95%\n",
      "Weight has been saved !!\n",
      "epoch: 12, train loss, test loss , accuracy |0.21916672806421186 , 0.47689836766335103, 95.24000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 13, train loss, test loss , accuracy |0.1678976429771529 , 0.4529230950976664, 95.49%\n",
      "Weight has been saved !!\n",
      "epoch: 14, train loss, test loss , accuracy |0.19115793268029613 , 0.48745454087764084, 95.17%\n",
      "Weight has been saved !!\n",
      "epoch: 15, train loss, test loss , accuracy |0.17077512605679365 , 0.4319238839532771, 95.52000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 16, train loss, test loss , accuracy |0.1678967971407306 , 0.47818376083548647, 95.38%\n",
      "Weight has been saved !!\n",
      "epoch: 17, train loss, test loss , accuracy |0.16789673815516495 , 0.46587944820444843, 95.49%\n",
      "Weight has been saved !!\n",
      "epoch: 18, train loss, test loss , accuracy |0.16831952543812476 , 0.4596751801083587, 95.5%\n",
      "Weight has been saved !!\n",
      "epoch: 19, train loss, test loss , accuracy |0.16837739859641912 , 0.452440166548707, 95.48%\n",
      "Weight has been saved !!\n",
      "epoch: 20, train loss, test loss , accuracy |0.1680822785784375 , 0.47582066914921206, 95.34%\n",
      "Weight has been saved !!\n",
      "epoch: 21, train loss, test loss , accuracy |0.16789678022075635 , 0.4378214584207301, 95.71%\n",
      "Weight has been saved !!\n",
      "epoch: 22, train loss, test loss , accuracy |0.16794099303158108 , 0.4604998882790827, 95.67%\n",
      "Weight has been saved !!\n",
      "epoch: 23, train loss, test loss , accuracy |0.16789677845920292 , 0.466672609004454, 95.67999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 24, train loss, test loss , accuracy |0.1680185185822346 , 0.45593918232186026, 95.63000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 25, train loss, test loss , accuracy |0.16792351255765384 , 0.4567576893883728, 95.78%\n",
      "Weight has been saved !!\n",
      "epoch: 26, train loss, test loss , accuracy |0.16789681252667557 , 0.4683492906130812, 95.78%\n",
      "Weight has been saved !!\n",
      "epoch: 27, train loss, test loss , accuracy |0.16789674887660158 , 0.45305485059867634, 95.89%\n",
      "Weight has been saved !!\n",
      "epoch: 28, train loss, test loss , accuracy |0.3550916052339548 , 0.45862071875461596, 95.73%\n",
      "Weight has been saved !!\n",
      "epoch: 29, train loss, test loss , accuracy |0.18299308802725492 , 0.4760424265385257, 95.78%\n",
      "Weight has been saved !!\n",
      "epoch: 30, train loss, test loss , accuracy |0.1678967721610367 , 0.43881457667155255, 95.92%\n",
      "Weight has been saved !!\n",
      "epoch: 31, train loss, test loss , accuracy |0.30027726878694244 , 0.42987804844051025, 96.09%\n",
      "Weight has been saved !!\n",
      "epoch: 32, train loss, test loss , accuracy |0.1774849540988497 , 0.4496815294124108, 95.74000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 33, train loss, test loss , accuracy |0.1680252960114675 , 0.46516931472596945, 95.65%\n",
      "Weight has been saved !!\n",
      "epoch: 34, train loss, test loss , accuracy |0.16789794634761288 , 0.45865261408884944, 95.73%\n",
      "Weight has been saved !!\n",
      "epoch: 35, train loss, test loss , accuracy |0.4321651079989571 , 0.4487229308725462, 95.75%\n",
      "Weight has been saved !!\n",
      "epoch: 36, train loss, test loss , accuracy |0.1982232483903498 , 0.4598542533169435, 95.72%\n",
      "Weight has been saved !!\n",
      "epoch: 37, train loss, test loss , accuracy |0.16789781877809504 , 0.4880369619450696, 95.62%\n",
      "Weight has been saved !!\n",
      "epoch: 38, train loss, test loss , accuracy |0.21482182754419243 , 0.4953699228387208, 95.48%\n",
      "Weight has been saved !!\n",
      "epoch: 39, train loss, test loss , accuracy |0.16834527628308513 , 0.49190399536928364, 95.67999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 40, train loss, test loss , accuracy |0.16790032752219783 , 0.5001216348490453, 95.61%\n",
      "Weight has been saved !!\n",
      "epoch: 41, train loss, test loss , accuracy |0.16789698347124019 , 0.4448817354040866, 95.92%\n",
      "Weight has been saved !!\n",
      "epoch: 42, train loss, test loss , accuracy |0.16823426312712106 , 0.45070563715921663, 96.08%\n",
      "Weight has been saved !!\n",
      "epoch: 43, train loss, test loss , accuracy |0.170758868264699 , 0.4541737858360679, 95.76%\n",
      "Weight has been saved !!\n",
      "epoch: 44, train loss, test loss , accuracy |0.16997932527359352 , 0.4380318761947634, 96.08%\n",
      "Weight has been saved !!\n",
      "epoch: 45, train loss, test loss , accuracy |0.16851327316071588 , 0.470246377603843, 95.81%\n",
      "Weight has been saved !!\n",
      "epoch: 46, train loss, test loss , accuracy |0.16790681153618572 , 0.4697600385528813, 95.95%\n",
      "Weight has been saved !!\n",
      "epoch: 47, train loss, test loss , accuracy |0.16789709555570342 , 0.46550896661003915, 95.8%\n",
      "Weight has been saved !!\n",
      "epoch: 48, train loss, test loss , accuracy |0.16789985517509406 , 0.4485470934155688, 96.00999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 49, train loss, test loss , accuracy |0.16790405990443658 , 0.4495392511189896, 95.93%\n",
      "Weight has been saved !!\n",
      "epoch: 50, train loss, test loss , accuracy |0.16789690052710235 , 0.47134609114323955, 95.94%\n",
      "Weight has been saved !!\n",
      "epoch: 51, train loss, test loss , accuracy |0.2789805196942428 , 0.453911184526749, 95.94%\n",
      "Weight has been saved !!\n",
      "epoch: 52, train loss, test loss , accuracy |0.1678999707517377 , 0.4325366866448219, 96.07%\n",
      "Weight has been saved !!\n",
      "epoch: 53, train loss, test loss , accuracy |0.16789678354329396 , 0.4430348862403313, 96.06%\n",
      "Weight has been saved !!\n",
      "epoch: 54, train loss, test loss , accuracy |0.16789735077603674 , 0.48437095729207175, 95.75%\n",
      "Weight has been saved !!\n",
      "epoch: 55, train loss, test loss , accuracy |0.1678969008412227 , 0.4674937440631372, 95.84%\n",
      "Weight has been saved !!\n",
      "epoch: 56, train loss, test loss , accuracy |0.30115913346485507 , 0.46653725797511525, 95.87%\n",
      "Weight has been saved !!\n",
      "epoch: 57, train loss, test loss , accuracy |0.16792025743522446 , 0.42616357707776426, 96.23%\n",
      "Weight has been saved !!\n",
      "epoch: 58, train loss, test loss , accuracy |0.28245549010592863 , 0.48264021255814343, 95.86%\n",
      "Weight has been saved !!\n",
      "epoch: 59, train loss, test loss , accuracy |0.24120191983122932 , 0.44692739793609787, 96.11%\n",
      "Weight has been saved !!\n",
      "epoch: 60, train loss, test loss , accuracy |0.22659950439875173 , 0.4770115287714701, 96.04%\n",
      "Weight has been saved !!\n",
      "epoch: 61, train loss, test loss , accuracy |0.1681147471235129 , 0.45961850184511566, 95.92%\n",
      "Weight has been saved !!\n",
      "epoch: 62, train loss, test loss , accuracy |0.17045593413759355 , 0.45211376486472055, 96.0%\n",
      "Weight has been saved !!\n",
      "epoch: 63, train loss, test loss , accuracy |0.2523817454419679 , 0.47611200131535686, 95.89999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 64, train loss, test loss , accuracy |0.2817597275725889 , 0.47179291048196575, 95.77%\n",
      "Weight has been saved !!\n",
      "epoch: 65, train loss, test loss , accuracy |0.2211909104046376 , 0.4408041647237789, 96.11%\n",
      "Weight has been saved !!\n",
      "epoch: 66, train loss, test loss , accuracy |0.19999673653870717 , 0.47063540118731945, 95.91%\n",
      "Weight has been saved !!\n",
      "epoch: 67, train loss, test loss , accuracy |0.167963397962148 , 0.4667000795796138, 95.99%\n",
      "Weight has been saved !!\n",
      "epoch: 68, train loss, test loss , accuracy |0.16789676535345532 , 0.4569298307665218, 96.09%\n",
      "Weight has been saved !!\n",
      "epoch: 69, train loss, test loss , accuracy |0.1678967311309465 , 0.46267558762307437, 95.97%\n",
      "Weight has been saved !!\n",
      "epoch: 70, train loss, test loss , accuracy |0.25252000085667875 , 0.48656449269674684, 96.0%\n",
      "Weight has been saved !!\n",
      "epoch: 71, train loss, test loss , accuracy |0.16798006895037432 , 0.49998350157699234, 95.73%\n",
      "Weight has been saved !!\n",
      "epoch: 72, train loss, test loss , accuracy |0.33579282381595615 , 0.4825253670967266, 95.97%\n",
      "Weight has been saved !!\n",
      "epoch: 73, train loss, test loss , accuracy |0.1679605444304231 , 0.5010317870273061, 95.74000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 74, train loss, test loss , accuracy |0.33494150185063387 , 0.48898697006279007, 95.88%\n",
      "Weight has been saved !!\n",
      "epoch: 75, train loss, test loss , accuracy |0.21070159494979615 , 0.4782705635778248, 96.02000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 76, train loss, test loss , accuracy |0.16789673291100649 , 0.47702656453288994, 96.06%\n",
      "Weight has been saved !!\n",
      "epoch: 77, train loss, test loss , accuracy |0.17759378108799254 , 0.4854004201425135, 96.11%\n",
      "Weight has been saved !!\n",
      "epoch: 78, train loss, test loss , accuracy |0.16789756614032933 , 0.4814712371230906, 96.0%\n",
      "Weight has been saved !!\n",
      "epoch: 79, train loss, test loss , accuracy |0.3356947694197607 , 0.5008540497758874, 95.84%\n",
      "Weight has been saved !!\n",
      "epoch: 80, train loss, test loss , accuracy |0.1889850927619332 , 0.4996396433008716, 95.62%\n",
      "Weight has been saved !!\n",
      "epoch: 81, train loss, test loss , accuracy |0.16792007529193687 , 0.5284126342962177, 95.77%\n",
      "Weight has been saved !!\n",
      "epoch: 82, train loss, test loss , accuracy |0.17637370887167858 , 0.4640118687170667, 95.96000000000001%\n",
      "Weight has been saved !!\n",
      "epoch: 83, train loss, test loss , accuracy |0.19224809971403467 , 0.48464133768679485, 95.99%\n",
      "Weight has been saved !!\n",
      "epoch: 84, train loss, test loss , accuracy |0.16789992677805934 , 0.48365507088231763, 95.97%\n",
      "Weight has been saved !!\n",
      "epoch: 85, train loss, test loss , accuracy |0.1678974520072352 , 0.4580320576818749, 96.19%\n",
      "Weight has been saved !!\n",
      "epoch: 86, train loss, test loss , accuracy |0.1678975147981788 , 0.4610168854270844, 96.06%\n",
      "Weight has been saved !!\n",
      "epoch: 87, train loss, test loss , accuracy |0.1679124850630885 , 0.45953997701396093, 96.11%\n",
      "Weight has been saved !!\n",
      "epoch: 88, train loss, test loss , accuracy |0.16802212376556633 , 0.4717505575061109, 96.15%\n",
      "Weight has been saved !!\n",
      "epoch: 89, train loss, test loss , accuracy |0.167948594311737 , 0.4664566513962725, 96.03%\n",
      "Weight has been saved !!\n",
      "epoch: 90, train loss, test loss , accuracy |0.17707035453875666 , 0.4564415656178957, 96.2%\n",
      "Weight has been saved !!\n",
      "epoch: 91, train loss, test loss , accuracy |0.30908149103995686 , 0.47323507234143963, 96.06%\n",
      "Weight has been saved !!\n",
      "epoch: 92, train loss, test loss , accuracy |0.16789673120254256 , 0.4764133826679163, 96.06%\n",
      "Weight has been saved !!\n",
      "epoch: 93, train loss, test loss , accuracy |0.16891941985343858 , 0.4687154865896813, 96.00999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 94, train loss, test loss , accuracy |0.17073839865489593 , 0.47484675613512345, 95.95%\n",
      "Weight has been saved !!\n",
      "epoch: 95, train loss, test loss , accuracy |0.18977421607536374 , 0.49340396564999756, 95.83%\n",
      "Weight has been saved !!\n",
      "epoch: 96, train loss, test loss , accuracy |0.16800463527482445 , 0.48509100364039087, 95.94%\n",
      "Weight has been saved !!\n",
      "epoch: 97, train loss, test loss , accuracy |0.16789673083974713 , 0.4734568427198241, 96.00999999999999%\n",
      "Weight has been saved !!\n",
      "epoch: 98, train loss, test loss , accuracy |0.16789676580372026 , 0.4622413710481114, 96.11%\n",
      "Weight has been saved !!\n",
      "epoch: 99, train loss, test loss , accuracy |0.23430880099372975 , 0.5016302218680067, 95.93%\n",
      "Weight has been saved !!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from math import gamma\n",
    "\n",
    "\n",
    "class NLayerMLP:\n",
    "\n",
    "    def __init__(self, sizes, weight_init_method):\n",
    "        # 存储权重值和偏置值\n",
    "        self.sizes = sizes\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for input_size, output_size in zip(sizes[0:-1], sizes[1:]):\n",
    "            # 初始化权重\n",
    "            if weight_init_method==\"0\":\n",
    "                # 使用0值初始化\n",
    "                weight = 0 * np.random.randn(input_size, output_size)\n",
    "            elif weight_init_method==\"Kaiming\":\n",
    "                # 使用Kaiming初始化\n",
    "                weight = 16*2/(input_size+output_size) * np.random.randn(input_size, output_size)\n",
    "            bias = np.zeros(output_size)\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "        # 这个神经网络的层数为n-1\n",
    "        size = len(sizes)-1\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        for i in range(0, size):\n",
    "            if i != size-1:\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "                #self.layers['Relu1'] = Relu()\n",
    "                self.layers[f'Sigmoid1{i}'] = Sigmoid()\n",
    "            else:\n",
    "                # 这里没有激活函数\n",
    "                self.layers[f'Affine{i}'] = Affine(self.weights[i], self.biases[i])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        #self.lastLayer = IdentityWithLoss()\n",
    "        self.params = dict()\n",
    "        self.params['sizes'] = self.sizes\n",
    "        self.params['biases'] = self.biases\n",
    "        self.params['weights'] = self.weights\n",
    "        self.params['layers'] = self.layers\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'] = numerical_gradient(loss_W, self.weights[i])\n",
    "            grads[f'b{i}'] = numerical_gradient(loss_W, self.biases[i])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        for i in range(0, len(self.sizes)-1):\n",
    "            grads[f'W{i}'], grads[f'b{i}'] = self.layers[f'Affine{i}'].dW, self.layers[f'Affine{i}'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def sgd(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None,\n",
    "            loss_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        for i in range(epochs):\n",
    "            for batch_index in range(0, train_size, batch_size):\n",
    "                lower_range = batch_index\n",
    "                upper_range = batch_index + batch_size\n",
    "                if upper_range > train_size:\n",
    "                    upper_range = train_size\n",
    "                x_batch = train_features[lower_range: upper_range, :]\n",
    "                y_batch = train_labels[lower_range: upper_range]\n",
    "                # 计算梯度\n",
    "                #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "                grad = self.gradient(x_batch, y_batch)\n",
    "                # 更新参数\n",
    "                for index in range(len(network.sizes)-1):\n",
    "                    self.weights[index] -= lr * grad[f\"W{index}\"]\n",
    "                    self.biases[index] -= lr * grad[f\"b{index}\"]\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "                if loss_path:\n",
    "                    with open(loss_path, \"a\") as f:\n",
    "                        if i == 0:\n",
    "                            f.write(\"epoch train_loss test_loss  accuracy\\n\")\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{loss}  {loss_test} {accuracy}\\n\")\n",
    "\n",
    "\n",
    "    def adam(self, training_data, epochs, batch_size, lr, test_data=None, iter_num=10, save_path=None,\n",
    "             loss_path=None):\n",
    "        train_features, train_labels = training_data\n",
    "        test_features, test_labels = test_data\n",
    "        train_size = len(train_labels)\n",
    "        test_size = len(test_labels)\n",
    "        gamma = 0.999\n",
    "        for i in range(epochs):\n",
    "            for batch_index in range(0, train_size, batch_size):\n",
    "                lower_range = batch_index\n",
    "                upper_range = batch_index + batch_size\n",
    "                if upper_range > train_size:\n",
    "                    upper_range = train_size\n",
    "                x_batch = train_features[lower_range: upper_range, :]\n",
    "                y_batch = train_labels[lower_range: upper_range]\n",
    "                # 计算梯度\n",
    "                #grad = self.numerical_gradient(x_batch, y_batch)\n",
    "                grad = self.gradient(x_batch, y_batch)\n",
    "                # 更新参数, 这里使用adam算法\n",
    "                rho1, rho2 = 0.9, 0.999\n",
    "                delta = 1e-8\n",
    "                for index in range(len(network.sizes)-1):\n",
    "                    weight = self.weights[index]\n",
    "                    weight_grad = grad[f\"W{index}\"]\n",
    "                    s = np.zeros_like(weight)\n",
    "                    r = np.zeros_like(weight)\n",
    "                    # update the first and the second moment\n",
    "                    s = rho1 * s + (1 - rho1) * weight_grad\n",
    "                    r = rho2 * r + (1 - rho2) * np.square(weight_grad)\n",
    "                    # get the partial first and second moment\n",
    "                    s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                    r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                    # use the partial first and second moment to modify the GD method\n",
    "                    self.weights[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "                    bias = self.biases[index]\n",
    "                    bias_grad = grad[f\"b{index}\"]\n",
    "                    s = np.zeros_like(bias)\n",
    "                    r = np.zeros_like(bias)\n",
    "                    # update the first and the second moment\n",
    "                    s = rho1 * s + (1 - rho1) * bias_grad\n",
    "                    r = rho2 * r + (1 - rho2) * np.square(bias_grad)\n",
    "                    # get the partial first and second moment\n",
    "                    s_hat = s / (1 - rho1 ** (i + 1))\n",
    "                    r_hat = r / (1 - rho1 ** (i + 1))\n",
    "                    # use the partial first and second moment to modify the GD method\n",
    "                    self.biases[index] -= lr * s_hat / (np.sqrt(r_hat) + delta)\n",
    "            if i%5==0:        \n",
    "              lr *= gamma\n",
    "\n",
    "            loss = self.loss(x_batch, y_batch)\n",
    "            loss_test = self.loss(test_images, test_labels)\n",
    "            accuracy = np.sum(self.predict(test_features).argmax(1) == test_labels.argmax(1))/test_size\n",
    "            #if i % iter_per_epoch == 0:\n",
    "            if i % iter_num == 0:\n",
    "                print(f\"epoch: {i}, train loss, test loss , accuracy |{loss} , {loss_test}, {accuracy*100}%\")\n",
    "                if save_path:\n",
    "                    self.save(save_path)\n",
    "                    print(\"Weight has been saved !!\")\n",
    "                if loss_path:\n",
    "                    with open(loss_path, \"a\") as f:\n",
    "                        if i == 0:\n",
    "                            f.write(\"epoch train_loss test_loss  accuracy\\n\")\n",
    "                            f.write(f\"{i}  {loss}  {loss_test} {accuracy}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"{i}  {loss} {loss_test} {accuracy}\\n\")\n",
    "\n",
    "    # 保存权重文件\n",
    "    def save(self,path):\n",
    "        with open(path,'wb') as f :\n",
    "            self.params['sizes'] = self.sizes\n",
    "            self.params['biases'] = self.biases\n",
    "            self.params['weights'] = self.weights\n",
    "            self.params['layers'] = self.layers\n",
    "            pickle.dump(self.params, f)\n",
    "\n",
    "    # 导入权重文件\n",
    "    def load(self,path):\n",
    "        with open(path,'rb') as f :\n",
    "            self.params = pickle.load(f)\n",
    "            self.sizes = self.params['sizes']\n",
    "            self.biases = self.params['biases']\n",
    "            self.weights = self.params['weights']\n",
    "            self.layers = self.params['layers']\n",
    "\n",
    "\n",
    "        #print(weight)\n",
    "\n",
    "# 定义神经网络架构,使用kaiming初始化\n",
    "network = NLayerMLP(sizes=[784, 64, 32, 10], weight_init_method=\"Kaiming\")\n",
    "weight_save_path  = \"weight_multi_layer_adam_step_lr.pickle\"\n",
    "loss_path = \"weight_multi_layer_adam_step_lr.txt\"\n",
    "epoches = 100  # 适当设定循环的次数\n",
    "train_size = train_images.shape[0]\n",
    "batch_size = 128\n",
    "learning_rate = 1e-2\n",
    "load_weight = False\n",
    "train_net = True\n",
    "if load_weight:\n",
    "    network.load(weight_save_path)\n",
    "if True:\n",
    "    accuracy = np.sum(network.predict(test_images).argmax(1) == test_labels.argmax(1))/testing_set_size\n",
    "    print(f\"epoch: , train loss, test loss , accuracy |, {accuracy*100}%\")\n",
    "    print(\"Weight has been saved !!\")\n",
    "network.adam(training_data=(train_images, train_labels), epochs=epoches, batch_size=batch_size,\n",
    "            lr=learning_rate, test_data=(test_images, test_labels), iter_num=1,\n",
    "            save_path=weight_save_path, loss_path=loss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae51ee3d492f24e83e77a52eb34bf16365894f8747390aa8e17995579dedf394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
