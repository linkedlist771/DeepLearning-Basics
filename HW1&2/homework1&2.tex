\documentclass{homework}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{subeqnarray}
\usepackage{cases}
\usepackage{booktabs}
\usepackage{fontspec}
\usepackage{listings}
\usepackage{blindtext}
\usepackage{hyperref}
%\author{12019311-丁力}
\class{深度学习基础}
\date{\today}
\title{ Homework 1 and 2 }
\address{东南大学九龙湖校区}

\graphicspath{{./media/}}

\begin{document} \maketitle

\question 实现正向传播和反向传播的推导


在进行证明前，需要先定义一些符号：
\begin{table}[htbp]
	\centering 
	\caption{\label{tab:test}符号说明}
	\begin{tabular}{lcr}
		\toprule
		符号含义 & 含义  \\
		\midrule
		$a^{(l)}$ & 第l层经过激活函数激活后的输出 \\

		$a^{(L)}$ & 最后一层输出，这时$y=a^{(L)}$ \\
		$x$ & 输入数据,并且$x=a^{(1)}$ \\
		$w_{ij}^{(l)}$ & 权重，l代表第l层，i代表第l层的第i个神经元，
		j代表第l-1层的第j个神经元 \\
		$b^{(L)}$ & 第l层的偏置 \\
		$z^{(l)}$ & 对第l-1层的输出进行仿射变化得到结果，也就是
		$z^{(l)} =w^{(l)}a^{(l-1)} +b^{(l)} $\\
		$\sigma_{(l)}$ & 第l层的激活函数，并且有$a^{(l)} = 
		\sigma_{(l)}(z^{(l)})$\\
		$\delta^{(l)}_j$ & 损失函数对于
		第l层第j个加权输出的偏导，也就是$\delta^{(l)}_j= 
		\frac{\partial E}{\partial z^{(l)}_j}$\\
		$t$ & 输出的监督值 \\
		$E(y,t)$ & 损失函数,这里才用L2范数，也就是$E(y,t) = 
		\frac{\sum_k(y_k-t_k)^2}{2}$ \\
		$\theta$ & 权重,就是w的总称 \\
		$L$ & 最后一层的神经网络的层数 \\
		\bottomrule
	\end{tabular}
\end{table}

正向传播和方向传播都是为了求解得到损失函数关于权重以及偏置的梯度，
采用的都是链式法则。
\iffalse
\noindent Rather than finding the shortest path between two points, suppose our car is low on gas, so we want to take the path that uses the least fuel. In the real world, navigation optimized for fuel consumption may take more steps to reach a destination \footnote{\href{https://blog.google/products/maps/3-new-ways-navigate-more-sustainably-maps/}{Google Maps Blog}}. 

Consider the same MDP, but with two new ``efficient actions'' -- move right or move down. For example, starting from state 3, you can either move to state 4 or 9. Once again, the actions are deterministic and always succeed unless you run into a wall. Attempting to move in the direction of a wall from a gray square using an efficient action results in you moving \textit{down} one square. For clarity, we will use separate symbols $r_s$ for the reward associated with an inefficient action (right $\&$ up, or  right $\&$ down) and $r_e$ for the reward associated with an efficient action.
\fi 

\begin{enumerate}
	\item  [(a)] 反向传播公式推导：
	
	由链式法则，可以得到:


	\begin{subequations}  
		\begin{numcases}{} 
			\frac{\partial E}{\partial w_{ij}^{(l)}}  = \frac{\partial E}{\partial z^{(l)} }\frac{\partial z^{(l)} }{\partial w_{ij}^{(l)}}           \\
			\frac{\partial E}{\partial b_{j}^{(l)}}  = \frac{\partial E}{\partial z^{(l)} }\frac{\partial z^{(l)} }{\partial b_{j}^{(l)}}              
		\end{numcases} 
	\end{subequations}
	
对于(1a)和(1b)式子，可以化为:


\begin{subequations}  
	\begin{numcases}{} 
		\frac{\partial E}{\partial w_{ij}^{(l)}}  =\delta^{(l)} \frac{\partial z^{(l)} }{\partial w_{ij}^{(l)}}           \\
		\frac{\partial E}{\partial b_{j}^{(l)}}  =\delta^{(l)}\frac{\partial z^{(l)} }{\partial b_{j}^{(l)}}              
	\end{numcases} 
\end{subequations}


所以，也就是求解$\delta^{(l)},\frac{\partial z^{(l)} }{\partial w_{ij}^{(l)}},\frac{\partial z^{(l)} }{\partial b_{j}^{(l)}} $
这三个变量。


对于该神经网络，假设我们使用的激活函数都为同一个激活函数，也就是
$\sigma = \sigma_{(l)}$ , 为了方便起见，后文中将激活函数写为：
$\sigma$。

首先先求解$\delta^{(l)}$:

\begin{itemize}
	\item  求解$\delta^{(l)}$:
	
	对于最后一层神经网络，我们从定义中知道：
	$$a^{(L)} = 
		\sigma(z^{(L)})$$

	那么对于最后一层:
	$$\delta^{(L)} = \frac{\partial E}{\partial z^{(L)}}
	= \frac{\partial 	\frac{\sum_k(y_k-a^{(L)}_k)^2}{2}}{
		\partial a^{(L)}
	} \frac{\partial a^{(L)}}{\partial z^{(L)}}$$

	也就是
	$$\delta^{(L)} =\sum_k(a^{(L)}-y_k) \sigma^\prime (z^{(L)}) $$

   当$1\leq l \leq L-1$时,我们这里采用链式法则来计算
   ，由于是反向传播，所以我们选择用l+1层的偏导来表示，
   也就是:
   $$\delta^{(l)}= 
   \frac{\partial E}{\partial z^{(l)}}
   =    \frac{\partial E}{\partial z^{(l+1)}}
   \frac{\partial z^{(l+1)}}{\partial z^{(l)}} 
   = \delta^{(l+1)} \frac{\partial z^{(l+1)}}{\partial z^{(l)}}  $$
   
   这是一个递推关系式，为了进一步求解，我们需要求解$\frac{\partial z^{(l+1)}}{\partial z^{(l)}} $
   从定义中可以知道:

   $$z^{(l+1)} =w^{(l+1)}a^{(l)} +b^{(l+1)}
   = w^{(l+1)}\sigma(z^{(l)}) +b^{(l+1)}$$

   所以有:

   $$\frac{\partial z^{(l+1)}}{\partial z^{(l)}} = 
   w^{(l+1)}\sigma^\prime (z^{(l)}) $$

   那么这样我们便可以得到$\delta^{(l)}$的表达式为:

   $$\delta^{(l)} = \sigma^\prime (z^{(l)}) (w^{(l+1)})^T
   \delta^{(l+1)} $$


   
至此我们完成了第一步，也就是
求解$\delta^{(l)}$
然后我们求解$\frac{\partial z^{(l)} }{\partial w_{ij}^{(l)}},\frac{\partial z^{(l)} }{\partial b_{j}^{(l)}} $
这两个即可，便可以完成反向传播的推导。


\item 求解$\frac{\partial z^{(l)} }{\partial w_{ij}^{(l)}}$

从定义中可以知道:

$$z^{(l+1)} =w^{(l+1)}a^{(l)} +b^{(l+1)}
$$

所以有:


$$ \frac{\partial z^{(l)} }{\partial w_{ij}^{(l)}} = a^{(l-1)} $$




\item 求解$\frac{\partial z^{(l)} }{\partial b_{j}^{(l)}}$


从定义中可以知道:

$$z^{(l+1)} =w^{(l+1)}a^{(l)} +b^{(l+1)}
$$

所以有:

$$\frac{\partial z^{(l)} }{\partial b_{j}^{(l)}} = 1$$

至此，我们便完成了反向传播的推导，也就是:


\begin{subequations}  
	\begin{numcases}{} 
		\frac{\partial E}{\partial w_{ij}^{(l)}}  =\delta^{(l)} a^{(l-1)}        \\
		\frac{\partial E}{\partial b_{j}^{(l)}}  =\delta^{(l)}            \\
		\mbox{其中}:   \delta^{(l)} = \begin{cases}
			 \sigma^\prime (z^{(l)}) (w^{(l+1)})^T
		\delta^{(l+1)} , \qquad l = 1...L-1 \\
		\sum_k(a^{(L)}-y_k) \sigma^\prime (z^{(L)})  , \qquad l = L
		\end{cases}
	\end{numcases} 
\end{subequations}


\end{itemize}



	\item  [(b)] 正向传播公式推导：
	

	所谓正向传播，与上面不同的地方仅仅存在于关于$ \delta^{(l)}$
	求解过程中的链式法则使用上，这里将使用l-1层的偏导来表示，所以被称为正向传播。
	所以我们只需要改写其中关于$ \delta^{(l)}$的求解部分即可，其他部分与反向传播相同。

	
\begin{itemize}
	\item   对于第一层神经网络，我们从定义中知道：
	$$a^{(1)} = 
		x$$

	那么对于第一层:
	$$\delta^{(L)} = \frac{\partial E}{\partial z^{(L)}}
	= \frac{\partial E}{\partial a^{(L)}}
	\frac{\partial a^{(L)}}{\partial z^{(L)}} 
	= \frac{\partial E}{\partial x}
	\frac{\partial x}{\partial z^{(L)}} $$

	由于x是常数，所以$\frac{\partial x}{\partial z^{(L)}} = 0$也就是:

	$$\delta^{(L)} = 0 $$

   \item  当$2\leq l \leq L$时,我们这里采用链式法则来计算
   ，由于是正向传播，所以我们选择用l-1层的偏导来表示，
   也就是:
   $$\delta^{(l)}= 
   \frac{\partial E}{\partial z^{(l)}}
   =    \frac{\partial E}{\partial z^{(l-1)}}
   \frac{\partial z^{(l-1)}}{\partial z^{(l)}} 
   = \delta^{(l-1)} \frac{\partial z^{(l-1)}}{\partial z^{(l)}}  $$
   
   这是一个递推关系式，为了进一步求解，我们需要求解$\frac{\partial z^{(l+1)}}{\partial z^{(l)}} $
   从定义中可以知道:

   $$z^{(l+1)} =w^{(l+1)}a^{(l)} +b^{(l+1)}
   = w^{(l+1)}\sigma(z^{(l)}) +b^{(l+1)}$$

   所以有:

$$  z^{(l-1)} = \sigma^{-1}(\frac{z^{(l)}-b^{(l)}}{w^{(l)}})$$

那么可以得到:
   $$\frac{\partial z^{(l-1)}}{\partial z^{(l)}} = 
(\sigma^{-1}(\frac{z^{(l)}-b^{(l)}}{w^{(l)}}))^\prime$$

   那么这样我们便可以得到$\delta^{(l)}$的表达式为:

   $$\delta^{(l)} = 
   \delta^{(l-1)}(\sigma^{-1}(\frac{z^{(l)}-b^{(l)}}{w^{(l)}}))^\prime $$
\end{itemize}
	
\end{enumerate}

至此，我们便完成了正向传播的推导，也就是:


\begin{subequations}  
	\begin{numcases}{} 
		\frac{\partial E}{\partial w_{ij}^{(l)}}  =\delta^{(l)} a^{(l-1)}        \\
		\frac{\partial E}{\partial b_{j}^{(l)}}  =\delta^{(l)}            \\
		\mbox{其中}:   \delta^{(l)} = \begin{cases}
			\delta^{(l-1)}(\sigma^{-1}(\frac{z^{(l)}-b^{(l)}}{w^{(l)}}))^\prime  , \qquad l = 2...L \\
	0 , \qquad l = 1
		\end{cases}
	\end{numcases} 
\end{subequations}
\question 对于一个神经网络$y=DNN(x,\theta )$,试求解$\frac{\partial y}{\partial x}$


从上面，推导过程，不难看出，对于一个n层的神经网络，如果其输出为y，
那么有:

$$ y = DNN(x,\theta ) = 
\sigma(w^{(n)}\sigma(w^{(n-1)}\sigma(....)+b^{(n-1)})+b^{(n)})
$$


所以一个神经网络的输出对于其输入的偏导可以表示为:
$$\frac{\partial y}{\partial x} = 
\frac{\partial DNN(x,\theta )}{\partial x}
= \sigma^\prime(z^{(n)}+b)\times w^{(n)} \times 
\sigma^\prime(z^{(n-1)}+b)\times w^{(n-1)} \times \dots 
\times \sigma^\prime(w^{(1)}x+b^{(1)})\times w^{(1)}
$$

或者采用数值求导的方法,令$h$为一个较小的值，
那么：
$$\frac{\partial y}{\partial x} = 
\frac{\partial DNN(x,\theta )}{\partial x}
= \frac{DNN(x+h,\theta)-DNN(x-h,\theta)}{2h}
$$


\iffalse 

We proceed by factoring,
\begin{align*}
	x^2- 8x - 9     & = 9-9         &  & \text{Subtract 9 on both sides.}         \\
	x^2- x + 9x - 9 & = 0           &  & \text{Breaking the middle term.}         \\
	(x - 1)(x + 9)  & = 0           &  & \text{Pulling out common } (x - 1).      \\
	x               & \in \{1, -9\} &  & f(x)g(x) = 0 \Ra f(x) = 0 \vee g(x) = 0. \\
\end{align*}

\fig[0.3]{cipher.png, diagram.jpg}{Cipher wheels.}{wheel}

\question Figure \ref{wheel} shows two cipher wheels. The left one is from Jeffrey Hoffstein, et al. \cite{hoffstein2008introduction} (pg. 3). Write a Python 3 program that uses it to encrypt: \texttt{FOUR SCORE AND SEVEN YEARS AGO}.

\lstinputlisting[language=Python, caption={Python 3 implementing figure \ref{wheel} left wheel.}, label=gcd]{code/prog.py}

We get: \texttt{KTZW XHTWJ FSI XJAJS DJFWX FLT}.

\fi 


\title{ Homework 2}

\question 对于以下实现以下几种算法，并画出其对应的收敛情况。
\begin{enumerate}
	\item 1:gradient descend
	\item 2:Ada grad
	\item 3:RMS prop
	\item 4:Momentum
	\item 5:Nesterov
	\item  6:Adam
\end{enumerate}

对于各个算法的实现如下:

\lstinputlisting[language=Python, caption={基于python3对于6中优化算法的实现}, label=gcd]{E:/桌面/深度学习数学基础/DeepLearning-Basics/Optimize Algorithm/optimization_algorithms.py}


得到的迭代曲线如下:

\fig[1]{E:/桌面/深度学习数学基础/DeepLearning-Basics/Optimize Algorithm/迭代学习曲线3.png}
{The iteration curve for the six kinds of optimization algorithms}{wheel}



从迭代曲线不难看出，adam算法的收敛速度最快，其次的时RMS prop算法，
然后剩下四种算法速度都比较慢，其中Ada grad最快，剩下的动量法，
梯度下降以及Nesterov速度基本上一样，都比较慢。

速度较快的算法都是基于Ada grad来实现的，所以速度较快，所以Adam和RMS prop
算法速度较快，这两个都是基于Ada grad的变种。
而Nesterov算法实现时是对动量法增加了一个提前一步的迭代，都是基于最基本的
梯度下降的优化，所以最后的收敛速度都是比较慢的。


\question 对于一个隐藏层，输入层，输出层均为100的五层神经网络，
给予一个满足高斯分布的输入$x\in R^{1000\times 100}$分别讨论
以下情况的每层输出的分布,并画出直方图：
\begin{itemize}
	\item  $\sigma = \begin{cases}
  sigmoid \\ relu \end{cases}$
	\item  $\theta = \begin{cases}
		0 \\   \thicksim N(0, \delta) ,\qquad \delta = 1 \quad or \quad 0.01 \end{cases}$
\end{itemize}


对神经网络进行初始化，然后进行正向传播，得到不同参数的神经网络的每一个输出层的直方图如下:

由于输出的图片多达40个，所以这里给出图片的链接:

点击这个\href{http://101.132.169.133/nnFeature/}{超链接}查看所有输出图片。


从输出的图片的直方图可以看出：
\begin{enumerate}
	\item 当$\theta = 0$ 时，由于线性层的
	权值和偏置都是0，所以不管输入的值是多少进行加权求和以及
	激活后得到的值都是0。所以此时第1到第5层的输出层都是0。

	\item 当$\theta \thicksim N(0,1)$ 时,也就是其满足标准正态分布时：
	
	当激活函数为Sigmoid函数时：
	第一层$a_1$输出是基本满足单峰正态分布的，$a_2$的输出的分布就发生了部分的偏离。
	而$a_3，a_4$ 基本满足多峰正态分布，而对于$a_5$层就趋向于均匀分布了。

	当激活函数为Relu函数时： 对于所有的输出层，直方图中输出分布都是0值较多，
	其他值的数量都较少。

	而$\delta$影响的就是分布的方差，也就是分布的范围，当$\delta$较大时($\delta=1$)
	分布更广。

	\item 对于上述情况，应该是Relu函数对于小于0的输入输出为0所以导致了大部分的输出都是0，
	而Sigmoid对于正负的输入都可以激活，这里面的由于是标准正态分布，所以还没达到Sigmoid函数的饱和区。
\end{enumerate}

% citations
\bibliographystyle{plain}
\bibliography{citations}

\end{document}